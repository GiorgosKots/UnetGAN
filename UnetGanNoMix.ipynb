{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ba1d06-17af-4fca-a468-ca43e59c58a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import gumbel_softmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77120cea-ca66-479f-88d2-dba34d91bbc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2600 sequences\n",
      "First sequence shape: torch.Size([156, 5])\n",
      "\n",
      "Mapping: {'P': 0, 'A': 1, 'T': 2, 'G': 3, 'C': 4}\n",
      "\n",
      "Number of sequences: 2600\n",
      "Batch shape: torch.Size([64, 156, 5])\n",
      "\n",
      "Sample from batch (showing where P padding is):\n",
      "tensor([1, 2, 3, 2, 2, 4, 2, 2, 3, 1])\n",
      "Successfully loaded 2600 sequences\n"
     ]
    }
   ],
   "source": [
    "# Or Option 2: Use absolute path\n",
    "file_path = r\"C:\\Users\\kotsgeo\\Documents\\GANs\\AMPdata.txt\"\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, file_path, seq_length=156):\n",
    "        self.seq_length = seq_length\n",
    "        self.sequences = []\n",
    "        self.char_to_idx = {'P': 0, 'A': 1, 'T': 2, 'G': 3, 'C': 4}\n",
    "        \n",
    "        # Handle both absolute and relative paths\n",
    "        if not os.path.isabs(file_path):\n",
    "            file_path = os.path.join(os.getcwd(), file_path)\n",
    "            \n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    seq = line.strip().split('\\t')[0]\n",
    "                    if len(seq) < seq_length:\n",
    "                        seq = seq + 'P' * (seq_length - len(seq))\n",
    "                    seq = seq[:seq_length]\n",
    "                    self.sequences.append(seq)\n",
    "            print(f\"Successfully loaded {len(self.sequences)} sequences\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Data file not found at: {file_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        one_hot = torch.zeros(self.seq_length, len(self.char_to_idx))\n",
    "        for i, char in enumerate(seq[:self.seq_length]):\n",
    "            if char in self.char_to_idx:\n",
    "                one_hot[i][self.char_to_idx[char]] = 1\n",
    "            else:\n",
    "                one_hot[i][self.char_to_idx['P']] = 1\n",
    "        return one_hot\n",
    "\n",
    "# Test the dataset\n",
    "if __name__ == \"__main__\":\n",
    "    # You can use either relative or absolute path\n",
    "    # Option 1: Relative path\n",
    "    dataset = SequenceDataset(\"AMPdata.txt\")\n",
    "    \n",
    "    # Option 2: Absolute path\n",
    "    # dataset = SequenceDataset(\"/path/to/your/AMPdata.txt\")\n",
    "\n",
    "    # Print first sequence\n",
    "    first_seq = dataset[0]\n",
    "    print(\"First sequence shape:\", first_seq.shape)\n",
    "    print(\"\\nMapping:\", dataset.char_to_idx)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "    print('\\nNumber of sequences:', len(dataloader.dataset))\n",
    "    \n",
    "    # Check a batch\n",
    "    for batch in dataloader:\n",
    "        print(\"Batch shape:\", batch.shape)\n",
    "        print(\"\\nSample from batch (showing where P padding is):\")\n",
    "        print(torch.argmax(batch[0], dim=1)[:10])  \n",
    "        break\n",
    "dataset = SequenceDataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447f5a11-5fb4-459c-a661-d3fc86a27158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, hidden):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.hidden_channels = hidden // 4\n",
    "        \n",
    "        # Down projection\n",
    "        self.down_proj = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(hidden, self.hidden_channels, 1, padding=0)\n",
    "        )\n",
    "        \n",
    "        # Main processing\n",
    "        self.main_proc = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.hidden_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(self.hidden_channels, self.hidden_channels, 5, padding=2),\n",
    "            \n",
    "            nn.BatchNorm1d(self.hidden_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(self.hidden_channels, self.hidden_channels, 5, padding=2)\n",
    "        )\n",
    "        \n",
    "        # Up projection\n",
    "        self.up_proj = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.hidden_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(self.hidden_channels, hidden, 1, padding=0),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.down_proj(x)\n",
    "        h = self.main_proc(h)\n",
    "        h = self.up_proj(h)\n",
    "        return x + h\n",
    "    \n",
    "class ImprovedGenerator(nn.Module):\n",
    "    def __init__(self, n_chars, seq_len, batch_size, hidden):\n",
    "        super(ImprovedGenerator, self).__init__()\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.n_chars = n_chars\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden = hidden\n",
    "        \n",
    "        # Start from length 5\n",
    "        self.bottom_length = 5\n",
    "        \n",
    "        # Initial projection\n",
    "        self.linear = nn.Linear(128, hidden*16*self.bottom_length)\n",
    "        \n",
    "        # Progressive upsampling blocks\n",
    "        # 5 -> 10 -> 20 -> 40 -> 80 -> 156\n",
    "        self.upscale_factors = [2, 2, 2, 2, 1.95]  # Last step adjusts to reach exactly 156\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            # First block: maintain channels\n",
    "            nn.Sequential(\n",
    "                ResBlock(hidden*16),              \n",
    "                nn.Upsample(scale_factor=self.upscale_factors[0], mode='nearest')  # 5 -> 10\n",
    "            ),\n",
    "            \n",
    "            # Second block: 16ch → 8ch\n",
    "            nn.Sequential(\n",
    "                ResBlock(hidden*16),          \n",
    "                nn.Conv1d(hidden*16, hidden*8, 1),\n",
    "                nn.Upsample(scale_factor=self.upscale_factors[1])  # 10 -> 20\n",
    "            ),\n",
    "            \n",
    "            # Third block: 8ch → 4ch\n",
    "            nn.Sequential(\n",
    "                ResBlock(hidden*8),           \n",
    "                nn.Conv1d(hidden*8, hidden*4, 1),\n",
    "                nn.Upsample(scale_factor=self.upscale_factors[2])  # 20 -> 40\n",
    "            ),\n",
    "            \n",
    "            # Fourth block: 4ch → 2ch\n",
    "            nn.Sequential(\n",
    "                ResBlock(hidden*4),           \n",
    "                nn.Conv1d(hidden*4, hidden*2, 1),\n",
    "                nn.Upsample(scale_factor=self.upscale_factors[3])  # 40 -> 80\n",
    "            ),\n",
    "            \n",
    "            # Fifth block: 2ch → ch\n",
    "            nn.Sequential(\n",
    "                ResBlock(hidden*2),           \n",
    "                nn.Conv1d(hidden*2, hidden, 1),\n",
    "                nn.Upsample(scale_factor=self.upscale_factors[4])  # 80 -> 156\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Final layers\n",
    "        self.final = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden, n_chars, 3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        # Initial projection and reshape\n",
    "        output = self.linear(noise)  # [64, 5120]\n",
    "        output = output.view(-1, self.hidden*16, self.bottom_length)  # [64, 1024, 5]\n",
    "        \n",
    "        # Through progressive blocks\n",
    "        for block in self.blocks:\n",
    "            output = block(output)\n",
    "            \n",
    "        # Final processing\n",
    "        output = self.final(output)  # [64, 5, 156]\n",
    "        \n",
    "        # Prepare for Gumbel\n",
    "        output = output.transpose(1, 2)  # [64, 156, 5]\n",
    "        shape = output.size()\n",
    "        output = output.contiguous()\n",
    "        output = output.view(-1, self.n_chars)  # [9984, 5]\n",
    "        output = gumbel_softmax(output, 0.5)\n",
    "        \n",
    "        return output.view(shape)  # Back to [64, 156, 5]\n",
    "\n",
    "class UNetDiscriminator(nn.Module):\n",
    "    def __init__(self, n_chars, seq_len, batch_size, hidden):\n",
    "        super(UNetDiscriminator, self).__init__()\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.n_chars = n_chars\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # Initial projection\n",
    "        self.initial_conv = nn.Conv1d(n_chars, hidden, 1)  # ch\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            ResBlock(hidden),              # 64\n",
    "            nn.Conv1d(hidden, hidden*2, 1),# -> 128\n",
    "            nn.AvgPool1d(2)               # 156 -> 78\n",
    "        )\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            ResBlock(hidden*2),           # 128\n",
    "            nn.Conv1d(hidden*2, hidden*4, 1),# -> 256\n",
    "            nn.AvgPool1d(2)               # 78 -> 39\n",
    "        )\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            ResBlock(hidden*4),           # 256\n",
    "            nn.Conv1d(hidden*4, hidden*8, 1),# -> 512\n",
    "            nn.AvgPool1d(2)               # 39 -> 19\n",
    "        )\n",
    "\n",
    "        self.enc4 = nn.Sequential(\n",
    "            ResBlock(hidden*8),           # 512\n",
    "            nn.Conv1d(hidden*8, hidden*16, 1),# -> 1024\n",
    "            nn.AvgPool1d(2)               # 19 -> 9\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResBlock(hidden*16)  # 1024\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            ResBlock(hidden*16),          # 1024\n",
    "            nn.Conv1d(hidden*16, hidden*8, 1),# -> 512\n",
    "            nn.Upsample(scale_factor=2)    # 9 -> 18\n",
    "        )\n",
    "        self.dec1_res = ResBlock(hidden*16)  # After concat (512+512=1024)\n",
    "\n",
    "        self.dec2 = nn.Sequential(\n",
    "            ResBlock(hidden*16),          # 1024\n",
    "            nn.Conv1d(hidden*16, hidden*8, 1),# -> 512\n",
    "            nn.Upsample(scale_factor=2)    # 19 -> 38\n",
    "        )\n",
    "        self.dec2_conv = nn.Conv1d(hidden*12, hidden*16, 1)  # 768->1024\n",
    "        self.dec2_res = ResBlock(hidden*16)  # After concat adjust\n",
    "\n",
    "        self.dec3 = nn.Sequential(\n",
    "            ResBlock(hidden*16),          # 1024\n",
    "            nn.Conv1d(hidden*16, hidden*4, 1),# -> 256\n",
    "            nn.Upsample(scale_factor=2)    # 39 -> 78\n",
    "        )\n",
    "        self.dec3_conv = nn.Conv1d(hidden*6, hidden*8, 1)  # 384->512\n",
    "        self.dec3_res = ResBlock(hidden*8)  # After concat adjust\n",
    "\n",
    "        self.dec4 = nn.Sequential(\n",
    "            ResBlock(hidden*8),           # 512\n",
    "            nn.Conv1d(hidden*8, hidden*2, 1),# -> 128\n",
    "            nn.Upsample(scale_factor=2)    # 78 -> 156\n",
    "        )\n",
    "        self.dec4_conv = nn.Conv1d(hidden*3, hidden*4, 1)  # 192->256\n",
    "        self.dec4_res = ResBlock(hidden*4)  # After concat adjust\n",
    "\n",
    "        # Global output (from bottleneck)\n",
    "        self.global_output = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),    \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden*16, 1)      \n",
    "        )\n",
    "\n",
    "        # Pixel-wise output\n",
    "        self.pixel_output = nn.Conv1d(hidden*4, 1, 1)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def pad_end(self, x, target_size):\n",
    "        \"\"\"Add padding at the end to reach target_size\"\"\"\n",
    "        diff = target_size - x.size(2)\n",
    "        if diff > 0:\n",
    "            return F.pad(x, (0, diff), mode='constant', value=0)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Input shape: (batch_size, seq_len, n_chars)\n",
    "        x = input.transpose(1, 2)  # (batch_size, n_chars, seq_len)\n",
    "\n",
    "        # Initial projection\n",
    "        x = self.initial_conv(x)  # [batch, hidden, 156]\n",
    "        residuals = [x]\n",
    "\n",
    "        # Encoder path\n",
    "        x = self.enc1(x)        # -> 78\n",
    "        residuals.append(x)\n",
    "        x = self.enc2(x)        # -> 39\n",
    "        residuals.append(x)\n",
    "        x = self.enc3(x)        # -> 19\n",
    "        residuals.append(x)\n",
    "        x = self.enc4(x)        # -> 9\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(x)  # Store bottleneck output\n",
    "\n",
    "        # Global output\n",
    "        global_output = self.global_output(bottleneck)\n",
    "        global_output = torch.sigmoid(global_output)\n",
    "\n",
    "        # Decoder path with padding and skip connections\n",
    "        x = self.dec1(bottleneck)  # 9 -> 18\n",
    "        x = self.pad_end(x, 19)    # pad to 19\n",
    "        x = torch.cat([x, residuals.pop()], dim=1)\n",
    "        x = self.dec1_res(x)\n",
    "\n",
    "        x = self.dec2(x)           # 19 -> 38\n",
    "        x = self.pad_end(x, 39)    # pad to 39\n",
    "        x = torch.cat([x, residuals.pop()], dim=1)\n",
    "        x = self.dec2_conv(x)      # Adjust channels\n",
    "        x = self.dec2_res(x)\n",
    "\n",
    "        x = self.dec3(x)           # 39 -> 78\n",
    "        x = torch.cat([x, residuals.pop()], dim=1)\n",
    "        x = self.dec3_conv(x)      # Adjust channels\n",
    "        x = self.dec3_res(x)\n",
    "\n",
    "        x = self.dec4(x)           # 78 -> 156\n",
    "        x = torch.cat([x, residuals.pop()], dim=1)\n",
    "        x = self.dec4_conv(x)      # Adjust channels\n",
    "        x = self.dec4_res(x)\n",
    "\n",
    "        # Pixel-wise output\n",
    "        pixel_output = self.pixel_output(x)\n",
    "        pixel_output = torch.sigmoid(pixel_output)\n",
    "        pixel_output = pixel_output.transpose(1, 2)\n",
    "\n",
    "        return global_output, pixel_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fecceac-2424-4b91-b6ce-4eceb55b9b56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbdf95-b49e-4b20-a6f5-377193280f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seq_analysis import sample_and_analyze, analyze_sequences, save_analysis\n",
    "from JSD import jsd\n",
    "# from FRED import FREDCalculator\n",
    "\n",
    "# Initialize FReD calculator\n",
    "# fred_calculator = FREDCalculator()\n",
    "\n",
    "# Define results directory with absolute path\n",
    "results_dir = r\"C:\\Users\\kotsgeo\\Documents\\GANs\\GANs\\ResultsNoMix\"\n",
    "\n",
    "# Parameters\n",
    "n_chars = 5\n",
    "seq_len = 156\n",
    "batch_size = 64\n",
    "hidden_g = 64\n",
    "hidden_d = 64\n",
    "num_epochs = 70\n",
    "\n",
    "generator = ImprovedGenerator(n_chars, seq_len, batch_size, hidden_g).to(device)\n",
    "discriminator = UNetDiscriminator(n_chars, seq_len, batch_size, hidden_d).to(device)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.000005, betas=(0.5, 0.9))\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.00002, betas=(0.5, 0.9))\n",
    "\n",
    "d_scheduler = CosineAnnealingLR(d_optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "g_scheduler = CosineAnnealingLR(g_optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "def train_step(discriminator, real_sequences, fake_sequences, optimizer):\n",
    "    batch_size = real_sequences.size(0)\n",
    "\n",
    "    # Get predictions for real and fake\n",
    "    real_global, real_pixel = discriminator(real_sequences)\n",
    "    fake_global, fake_pixel = discriminator(fake_sequences)\n",
    "\n",
    "    # Calculate losses\n",
    "    enc_loss = -torch.mean(torch.log(real_global + 1e-8) + # Loss for the binary classifcation as Real or Fake for sequences\n",
    "                      torch.log(1 - fake_global + 1e-8))\n",
    "\n",
    "    dec_loss = -torch.mean(\n",
    "        torch.log(real_pixel + 1e-8) + # Real positions should be 1\n",
    "        torch.log(1 - fake_pixel + 1e-8))\n",
    "\n",
    "    # Total discriminator loss\n",
    "    total_loss = enc_loss + dec_loss \n",
    "\n",
    "    # Update weights\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward(retain_graph=True)\n",
    "\n",
    "    # Add gradient clipping here\n",
    "    torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return {\n",
    "        'enc_loss': enc_loss.item(),\n",
    "        'dec_loss': dec_loss.item(),\n",
    "        'total_loss': total_loss.item()\n",
    "    }\n",
    "\n",
    "def generator_step(generator, discriminator, batch_size, optimizer):\n",
    "    # Generate fake sequences\n",
    "    noise = torch.randn(batch_size, 128).to(device)\n",
    "    fake_sequences = generator(noise)\n",
    "\n",
    "    # Get discriminator predictions\n",
    "    fake_global, fake_pixel = discriminator(fake_sequences)\n",
    "\n",
    "    # Generator loss\n",
    "\n",
    "    # Diversity penalty\n",
    "    diversity_loss = -torch.mean(torch.abs(torch.diff(fake_sequences, dim=1)))\n",
    "\n",
    "    # Global loss\n",
    "    g_global_loss = -torch.mean(torch.log(fake_global + 1e-8))\n",
    "\n",
    "    # Pixel-wise loss\n",
    "    g_pixel_loss = -torch.mean(torch.log(fake_pixel + 1e-8))\n",
    "\n",
    "    # Total generator loss\n",
    "    g_loss = g_global_loss + g_pixel_loss #+ 1*diversity_loss\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "\n",
    "    # Add gradient clipping here\n",
    "    torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    return {\n",
    "        'g_global_loss': g_global_loss.item(),\n",
    "        'g_pixel_loss': g_pixel_loss.item(),\n",
    "        'g_total_loss': g_loss.item()\n",
    "    }\n",
    "\n",
    "def train(generator, discriminator, dataloader, num_epochs, d_step, g_step=1):\n",
    "    # Initialize JSD score\n",
    "    jsd_history = []\n",
    "    # best_jsd = float('inf')\n",
    "    \n",
    "    # Initialize FReD score tracking\n",
    "    # fred_history = []\n",
    "    # best_fred = float('inf')\n",
    "\n",
    "    # Initialize lists to store losses for each iteration\n",
    "    iteration_losses = {\n",
    "        # Discriminator Losses\n",
    "        'total_d_loss': [],\n",
    "        'enc_loss': [],\n",
    "        'dec_loss': [],\n",
    "\n",
    "        # Generator Losses\n",
    "        'total_g_loss': [],\n",
    "        'g_global_loss': [],\n",
    "        'g_pixel_loss': [],\n",
    "    }\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    # Add time tracking\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Initialize running averages\n",
    "        running_losses = {\n",
    "            'total_d_loss': 0,\n",
    "            'enc_loss': 0,\n",
    "            'dec_loss': 0,\n",
    "            'total_g_loss': 0,\n",
    "            'g_global_loss': 0,\n",
    "            'g_pixel_loss': 0\n",
    "        }\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        # Initialize lists to store real and generated sequences for FReD calculation\n",
    "        # real_seqs_for_fred = []\n",
    "        # generated_seqs_for_fred = []\n",
    "\n",
    "        for batch_idx, real_sequences in enumerate(dataloader):\n",
    "            total_iterations += 1\n",
    "\n",
    "            real_sequences = real_sequences.to(device)\n",
    "            batch_size = real_sequences.size(0)\n",
    "\n",
    "            # Generate fake sequences\n",
    "            noise = torch.randn(batch_size, 128).to(device)\n",
    "            fake_sequences = generator(noise)\n",
    "\n",
    "            # # Decode sequences for analysis and FReD calculation\n",
    "            # inv_charmap = {0:'P', 1:'A', 2:'T', 3:'G', 4:'C'}\n",
    "\n",
    "            # def decode_sequence(seq):\n",
    "            #     indices = np.argmax(seq.detach().cpu().numpy(), axis=1)\n",
    "            #     return ''.join([inv_charmap[idx] for idx in indices])\n",
    "\n",
    "            # # Decode real sequences\n",
    "            # real_decoded = [decode_sequence(seq) for seq in real_sequences]\n",
    "\n",
    "            # # Decode generated sequences\n",
    "            # generated_decoded = [decode_sequence(seq) for seq in fake_sequences]\n",
    "\n",
    "            # # Store sequences for FReD calculation (without padding)\n",
    "            # real_seqs_for_fred.extend([seq.replace('P', '') for seq in real_decoded])\n",
    "            # generated_seqs_for_fred.extend([seq.replace('P', '') for seq in generated_decoded])\n",
    "\n",
    "            # Train discriminator multiple times\n",
    "            d_losses_sum = {\n",
    "                'total_loss': 0,\n",
    "                'enc_loss': 0,\n",
    "                'dec_loss': 0,\n",
    "                'cons_loss': 0\n",
    "            }\n",
    "\n",
    "            # Train discriminator\n",
    "            for _ in range(d_step):\n",
    "                d_losses = train_step(discriminator,\n",
    "                                    real_sequences=real_sequences,\n",
    "                                    fake_sequences=fake_sequences,\n",
    "                                    optimizer=d_optimizer)\n",
    "\n",
    "                # Sum up d_losses\n",
    "                for key in d_losses:\n",
    "                    d_losses_sum[key] += d_losses[key]\n",
    "            \n",
    "            # Average d_losses over d_steps\n",
    "            d_losses_avg = {k: v/d_step for k, v in d_losses_sum.items()}\n",
    "            \n",
    "            # Train generator multiple times\n",
    "            g_losses_sum = {\n",
    "                'g_total_loss': 0,\n",
    "                'g_global_loss': 0,\n",
    "                'g_pixel_loss': 0\n",
    "            }\n",
    "\n",
    "            # Train generator\n",
    "            for _ in range(g_step):\n",
    "                g_losses = generator_step(generator, discriminator, batch_size, g_optimizer)\n",
    "            \n",
    "                # Sum up g_losses\n",
    "                for key in g_losses:\n",
    "                    g_losses_sum[key] += g_losses[key]\n",
    "\n",
    "            # Average g_losses over g_steps\n",
    "            g_losses_avg = {k: v/g_step for k, v in g_losses_sum.items()}\n",
    "\n",
    "            # Store iteration losses (using averages)\n",
    "            iteration_losses['total_d_loss'].append(d_losses_avg['total_loss'])\n",
    "            iteration_losses['enc_loss'].append(d_losses_avg['enc_loss'])\n",
    "            iteration_losses['dec_loss'].append(d_losses_avg['dec_loss'])\n",
    "            iteration_losses['total_g_loss'].append(g_losses_avg['g_total_loss'])\n",
    "            iteration_losses['g_global_loss'].append(g_losses_avg['g_global_loss'])\n",
    "            iteration_losses['g_pixel_loss'].append(g_losses_avg['g_pixel_loss'])\n",
    "\n",
    "            # Update running averages\n",
    "            running_losses['total_d_loss'] += d_losses_avg['total_loss']\n",
    "            running_losses['enc_loss'] += d_losses_avg['enc_loss']\n",
    "            running_losses['dec_loss'] += d_losses_avg['dec_loss']\n",
    "            running_losses['total_g_loss'] += g_losses_avg['g_total_loss']\n",
    "            running_losses['g_global_loss'] += g_losses_avg['g_global_loss']\n",
    "            running_losses['g_pixel_loss'] += g_losses_avg['g_pixel_loss']\n",
    "\n",
    "            # Print batch progress\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f'Batch [{batch_idx+1}/{num_batches}]')\n",
    "                print(f'D_total_loss: {d_losses_avg[\"total_loss\"]:.4f}')\n",
    "                print(f'├─ Enc_loss: {d_losses_avg[\"enc_loss\"]:.4f}')\n",
    "                print(f'├─ Dec_loss: {d_losses_avg[\"dec_loss\"]:.4f}')\n",
    "                print(f'G_total_loss: {g_losses_avg[\"g_total_loss\"]:.4f}')\n",
    "                print(f'├─ G_global_loss: {g_losses_avg[\"g_global_loss\"]:.4f}')\n",
    "                print(f'└─ G_pixel_loss: {g_losses_avg[\"g_pixel_loss\"]:.4f}\\n')\n",
    "\n",
    "        # Calculate JSD every N epochs (e.g., 5)\n",
    "        if epoch % 1 == 0:\n",
    "            current_jsd = jsd(generator, dataloader, num_batches=5)\n",
    "            jsd_history.append(current_jsd)\n",
    "\n",
    "        # # Calculate FReD every N epochs (e.g., 5)\n",
    "        # if epoch % 1 == 0 and len(real_seqs_for_fred) > 0 and len(generated_seqs_for_fred) > 0:\n",
    "        #     current_fred = fred_calculator.calculate_fred_from_sequences(\n",
    "        #         real_seqs_for_fred[:min(100, len(real_seqs_for_fred))],  # Use up to 100 sequences\n",
    "        #         generated_seqs_for_fred[:min(100, len(generated_seqs_for_fred))]\n",
    "        #     )\n",
    "        #     fred_history.append(current_fred)\n",
    "                \n",
    "        # Calculate time for this epoch\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        # Calculate epoch averages\n",
    "        avg_losses = {k: v/num_batches for k, v in running_losses.items()}\n",
    "        \n",
    "        # Print epoch averages\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
    "            f'Epoch Time: {epoch_time:.2f}s - '\n",
    "            f'Total Time: {format_time(total_time)} ')\n",
    "        print(f'D_total_loss: {avg_losses[\"total_d_loss\"]:.4f}')\n",
    "        print(f'├─ Enc_loss: {avg_losses[\"enc_loss\"]:.4f}')\n",
    "        print(f'├─ Dec_loss: {avg_losses[\"dec_loss\"]:.4f}')\n",
    "        print(f'G_total_loss: {avg_losses[\"total_g_loss\"]:.4f}')\n",
    "        print(f'├─ G_global_loss: {avg_losses[\"g_global_loss\"]:.4f}')\n",
    "        print(f'└─ G_pixel_loss: {avg_losses[\"g_pixel_loss\"]:.4f}\\n')\n",
    "        print(f'Latest JSD Score: {current_jsd:.4f}')\n",
    "        # print(f'Latest FReD Score: {current_fred:.4f}')\n",
    "        print(50*\"-\")\n",
    "        \n",
    "        d_scheduler.step()        \n",
    "        g_scheduler.step()\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            generated_seqs = sample_and_analyze(generator, num_samples=5, epoch=epoch, batch_size=5, device=device)\n",
    "            # Save generated sequences for analysis\n",
    "            save_analysis(generated_seqs, epoch)\n",
    "\n",
    "    return iteration_losses, total_iterations, jsd_history#, fred_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebca89d5-d70e-4a2a-ac67-74874a2d9bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics(iteration_losses, jsd_history, total_iterations, num_epochs, dataloader_size):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    # First subplot for iteration losses\n",
    "    plt.subplot(3, 1, 1)\n",
    "    iterations = range(total_iterations)\n",
    "    plt.plot(iterations, iteration_losses['total_d_loss'], label='D Loss', color='blue')\n",
    "    plt.plot(iterations, iteration_losses['total_g_loss'], label='G Loss', color='red')\n",
    "    plt.title('Generator and Discriminator Losses per Iteration')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Second subplot for epoch losses\n",
    "    plt.subplot(3, 1, 2)\n",
    "    # Calculate average loss per epoch\n",
    "    epochs = range(num_epochs)\n",
    "    d_losses_per_epoch = []\n",
    "    g_losses_per_epoch = []\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        start_idx = epoch * dataloader_size\n",
    "        end_idx = (epoch + 1) * dataloader_size\n",
    "        \n",
    "        d_epoch_loss = np.mean(iteration_losses['total_d_loss'][start_idx:end_idx])\n",
    "        g_epoch_loss = np.mean(iteration_losses['total_g_loss'][start_idx:end_idx])\n",
    "        \n",
    "        d_losses_per_epoch.append(d_epoch_loss)\n",
    "        g_losses_per_epoch.append(g_epoch_loss)\n",
    "    \n",
    "    plt.plot(epochs, d_losses_per_epoch, label='D Loss', color='blue')\n",
    "    plt.plot(epochs, g_losses_per_epoch, label='G Loss', color='red')\n",
    "    plt.title('Generator and Discriminator Losses per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Third subplot for JSD with fixed y-axis\n",
    "    plt.subplot(3, 1, 3)\n",
    "    jsd_epochs = range(len(jsd_history))\n",
    "    plt.plot(jsd_epochs, jsd_history, 'g-', label='JSD Score')\n",
    "    plt.title('JSD Score Progress')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('JSD Score')\n",
    "    plt.ylim(0, 1)  # Set fixed y-axis range\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_metrics(iteration_losses, jsd_history, fred_history, total_iterations, num_epochs, dataloader_size):\n",
    "#     plt.figure(figsize=(12, 16))  # Increased height to accommodate 4 subplots\n",
    "\n",
    "#     # First subplot for iteration losses\n",
    "#     plt.subplot(4, 1, 1)\n",
    "#     iterations = range(total_iterations)\n",
    "#     plt.plot(iterations, iteration_losses['total_d_loss'], label='D Loss', color='blue')\n",
    "#     plt.plot(iterations, iteration_losses['total_g_loss'], label='G Loss', color='red')\n",
    "#     plt.title('Generator and Discriminator Losses per Iteration')\n",
    "#     plt.xlabel('Iteration')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     # Second subplot for epoch losses\n",
    "#     plt.subplot(4, 1, 2)\n",
    "#     # Calculate average loss per epoch\n",
    "#     epochs = range(num_epochs)\n",
    "#     d_losses_per_epoch = []\n",
    "#     g_losses_per_epoch = []\n",
    "\n",
    "#     for epoch in epochs:\n",
    "#         start_idx = epoch * dataloader_size\n",
    "#         end_idx = (epoch + 1) * dataloader_size\n",
    "\n",
    "#         d_epoch_loss = np.mean(iteration_losses['total_d_loss'][start_idx:end_idx])\n",
    "#         g_epoch_loss = np.mean(iteration_losses['total_g_loss'][start_idx:end_idx])\n",
    "\n",
    "#         d_losses_per_epoch.append(d_epoch_loss)\n",
    "#         g_losses_per_epoch.append(g_epoch_loss)\n",
    "\n",
    "#     plt.plot(epochs, d_losses_per_epoch, label='D Loss', color='blue')\n",
    "#     plt.plot(epochs, g_losses_per_epoch, label='G Loss', color='red')\n",
    "#     plt.title('Generator and Discriminator Losses per Epoch')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     # Third subplot for JSD with fixed y-axis\n",
    "#     plt.subplot(4, 1, 3)\n",
    "#     jsd_epochs = range(len(jsd_history))\n",
    "#     plt.plot(jsd_epochs, jsd_history, 'g-', label='JSD Score')\n",
    "#     plt.title('JSD Score Progress')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('JSD Score')\n",
    "#     plt.ylim(0, 1)  # Set fixed y-axis range\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     # Fourth subplot for FReD\n",
    "#     plt.subplot(4, 1, 4)\n",
    "#     fred_epochs = range(len(fred_history))\n",
    "#     plt.plot(fred_epochs, fred_history, 'm-', label='FReD Score')\n",
    "#     plt.title('FReD Score Progress')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('FReD Score')\n",
    "#     # Set y-axis range based on your observed FReD values\n",
    "#     # For example, if your FReD scores range between 0-100:\n",
    "#     plt.ylim(0, 100)  # Adjust this based on your actual FReD values\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb752029-111e-43db-b91e-b67059019258",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [1/40]\n",
      "D_total_loss: 7.2639\n",
      "├─ Enc_loss: 1.4468\n",
      "├─ Dec_loss: 5.8171\n",
      "G_total_loss: 1.4322\n",
      "├─ G_global_loss: 0.9023\n",
      "└─ G_pixel_loss: 0.5299\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 4.7786\n",
      "├─ Enc_loss: 1.4306\n",
      "├─ Dec_loss: 3.3481\n",
      "G_total_loss: 2.5278\n",
      "├─ G_global_loss: 0.8130\n",
      "└─ G_pixel_loss: 1.7148\n",
      "\n",
      "Epoch [1/70] - Epoch Time: 14.37s - Total Time: 00:00:14 \n",
      "D_total_loss: 4.8798\n",
      "├─ Enc_loss: 1.4289\n",
      "├─ Dec_loss: 3.4509\n",
      "G_total_loss: 2.4507\n",
      "├─ G_global_loss: 0.8189\n",
      "└─ G_pixel_loss: 1.6318\n",
      "\n",
      "Latest JSD Score: 0.5447\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 4.2904\n",
      "├─ Enc_loss: 1.4063\n",
      "├─ Dec_loss: 2.8841\n",
      "G_total_loss: 2.3495\n",
      "├─ G_global_loss: 0.7448\n",
      "└─ G_pixel_loss: 1.6047\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 3.7302\n",
      "├─ Enc_loss: 1.3726\n",
      "├─ Dec_loss: 2.3575\n",
      "G_total_loss: 2.4347\n",
      "├─ G_global_loss: 0.7223\n",
      "└─ G_pixel_loss: 1.7124\n",
      "\n",
      "Epoch [2/70] - Epoch Time: 14.89s - Total Time: 00:00:29 \n",
      "D_total_loss: 3.7609\n",
      "├─ Enc_loss: 1.3817\n",
      "├─ Dec_loss: 2.3792\n",
      "G_total_loss: 2.4004\n",
      "├─ G_global_loss: 0.7220\n",
      "└─ G_pixel_loss: 1.6785\n",
      "\n",
      "Latest JSD Score: 0.5396\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 3.3183\n",
      "├─ Enc_loss: 1.3614\n",
      "├─ Dec_loss: 1.9569\n",
      "G_total_loss: 2.5197\n",
      "├─ G_global_loss: 0.7131\n",
      "└─ G_pixel_loss: 1.8066\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 3.8237\n",
      "├─ Enc_loss: 1.3756\n",
      "├─ Dec_loss: 2.4481\n",
      "G_total_loss: 1.8882\n",
      "├─ G_global_loss: 0.6978\n",
      "└─ G_pixel_loss: 1.1904\n",
      "\n",
      "Epoch [3/70] - Epoch Time: 15.10s - Total Time: 00:00:44 \n",
      "D_total_loss: 3.6632\n",
      "├─ Enc_loss: 1.3725\n",
      "├─ Dec_loss: 2.2906\n",
      "G_total_loss: 2.0365\n",
      "├─ G_global_loss: 0.7006\n",
      "└─ G_pixel_loss: 1.3359\n",
      "\n",
      "Latest JSD Score: 0.6868\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 3.2725\n",
      "├─ Enc_loss: 1.3393\n",
      "├─ Dec_loss: 1.9332\n",
      "G_total_loss: 2.0469\n",
      "├─ G_global_loss: 0.7178\n",
      "└─ G_pixel_loss: 1.3290\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.9261\n",
      "├─ Enc_loss: 1.3280\n",
      "├─ Dec_loss: 1.5981\n",
      "G_total_loss: 2.2033\n",
      "├─ G_global_loss: 0.7239\n",
      "└─ G_pixel_loss: 1.4795\n",
      "\n",
      "Epoch [4/70] - Epoch Time: 15.26s - Total Time: 00:00:59 \n",
      "D_total_loss: 3.0163\n",
      "├─ Enc_loss: 1.3191\n",
      "├─ Dec_loss: 1.6972\n",
      "G_total_loss: 2.2464\n",
      "├─ G_global_loss: 0.7315\n",
      "└─ G_pixel_loss: 1.5149\n",
      "\n",
      "Latest JSD Score: 0.5428\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.8173\n",
      "├─ Enc_loss: 1.3020\n",
      "├─ Dec_loss: 1.5153\n",
      "G_total_loss: 2.3569\n",
      "├─ G_global_loss: 0.7448\n",
      "└─ G_pixel_loss: 1.6121\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 3.1479\n",
      "├─ Enc_loss: 1.3325\n",
      "├─ Dec_loss: 1.8154\n",
      "G_total_loss: 2.0320\n",
      "├─ G_global_loss: 0.7305\n",
      "└─ G_pixel_loss: 1.3014\n",
      "\n",
      "Epoch [5/70] - Epoch Time: 15.07s - Total Time: 00:01:14 \n",
      "D_total_loss: 3.0467\n",
      "├─ Enc_loss: 1.3069\n",
      "├─ Dec_loss: 1.7398\n",
      "G_total_loss: 2.1074\n",
      "├─ G_global_loss: 0.7353\n",
      "└─ G_pixel_loss: 1.3721\n",
      "\n",
      "Latest JSD Score: 0.5793\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.7287\n",
      "├─ Enc_loss: 1.2595\n",
      "├─ Dec_loss: 1.4692\n",
      "G_total_loss: 2.1922\n",
      "├─ G_global_loss: 0.7826\n",
      "└─ G_pixel_loss: 1.4096\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.4420\n",
      "├─ Enc_loss: 1.2514\n",
      "├─ Dec_loss: 1.1906\n",
      "G_total_loss: 2.5136\n",
      "├─ G_global_loss: 0.7791\n",
      "└─ G_pixel_loss: 1.7345\n",
      "\n",
      "Epoch [6/70] - Epoch Time: 14.21s - Total Time: 00:01:29 \n",
      "D_total_loss: 2.5685\n",
      "├─ Enc_loss: 1.2436\n",
      "├─ Dec_loss: 1.3249\n",
      "G_total_loss: 2.3580\n",
      "├─ G_global_loss: 0.7801\n",
      "└─ G_pixel_loss: 1.5779\n",
      "\n",
      "Latest JSD Score: 0.5475\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.5391\n",
      "├─ Enc_loss: 1.2344\n",
      "├─ Dec_loss: 1.3048\n",
      "G_total_loss: 2.5263\n",
      "├─ G_global_loss: 0.8014\n",
      "└─ G_pixel_loss: 1.7249\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.6987\n",
      "├─ Enc_loss: 1.2383\n",
      "├─ Dec_loss: 1.4604\n",
      "G_total_loss: 2.2448\n",
      "├─ G_global_loss: 0.7740\n",
      "└─ G_pixel_loss: 1.4708\n",
      "\n",
      "Epoch [7/70] - Epoch Time: 13.89s - Total Time: 00:01:42 \n",
      "D_total_loss: 2.6958\n",
      "├─ Enc_loss: 1.2391\n",
      "├─ Dec_loss: 1.4567\n",
      "G_total_loss: 2.2222\n",
      "├─ G_global_loss: 0.7716\n",
      "└─ G_pixel_loss: 1.4507\n",
      "\n",
      "Latest JSD Score: 0.6692\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.5772\n",
      "├─ Enc_loss: 1.2021\n",
      "├─ Dec_loss: 1.3751\n",
      "G_total_loss: 2.6407\n",
      "├─ G_global_loss: 0.8238\n",
      "└─ G_pixel_loss: 1.8168\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.1412\n",
      "├─ Enc_loss: 1.1633\n",
      "├─ Dec_loss: 0.9779\n",
      "G_total_loss: 2.4676\n",
      "├─ G_global_loss: 0.8200\n",
      "└─ G_pixel_loss: 1.6476\n",
      "\n",
      "Epoch [8/70] - Epoch Time: 14.06s - Total Time: 00:01:56 \n",
      "D_total_loss: 2.2685\n",
      "├─ Enc_loss: 1.1702\n",
      "├─ Dec_loss: 1.0983\n",
      "G_total_loss: 2.4718\n",
      "├─ G_global_loss: 0.8249\n",
      "└─ G_pixel_loss: 1.6469\n",
      "\n",
      "Latest JSD Score: 0.5418\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 1.9959\n",
      "├─ Enc_loss: 1.1271\n",
      "├─ Dec_loss: 0.8688\n",
      "G_total_loss: 2.8572\n",
      "├─ G_global_loss: 0.8386\n",
      "└─ G_pixel_loss: 2.0186\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 3.5327\n",
      "├─ Enc_loss: 1.3018\n",
      "├─ Dec_loss: 2.2308\n",
      "G_total_loss: 1.4939\n",
      "├─ G_global_loss: 0.6947\n",
      "└─ G_pixel_loss: 0.7992\n",
      "\n",
      "Epoch [9/70] - Epoch Time: 14.61s - Total Time: 00:02:11 \n",
      "D_total_loss: 3.0630\n",
      "├─ Enc_loss: 1.2698\n",
      "├─ Dec_loss: 1.7932\n",
      "G_total_loss: 1.8070\n",
      "├─ G_global_loss: 0.7157\n",
      "└─ G_pixel_loss: 1.0913\n",
      "\n",
      "Latest JSD Score: 0.6374\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.4503\n",
      "├─ Enc_loss: 1.1626\n",
      "├─ Dec_loss: 1.2876\n",
      "G_total_loss: 2.4243\n",
      "├─ G_global_loss: 0.8380\n",
      "└─ G_pixel_loss: 1.5863\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.3707\n",
      "├─ Enc_loss: 1.1637\n",
      "├─ Dec_loss: 1.2070\n",
      "G_total_loss: 2.3976\n",
      "├─ G_global_loss: 0.8298\n",
      "└─ G_pixel_loss: 1.5678\n",
      "\n",
      "Epoch [10/70] - Epoch Time: 15.30s - Total Time: 00:02:26 \n",
      "D_total_loss: 2.3330\n",
      "├─ Enc_loss: 1.1672\n",
      "├─ Dec_loss: 1.1658\n",
      "G_total_loss: 2.3095\n",
      "├─ G_global_loss: 0.8230\n",
      "└─ G_pixel_loss: 1.4864\n",
      "\n",
      "Latest JSD Score: 0.5310\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.1849\n",
      "├─ Enc_loss: 1.1138\n",
      "├─ Dec_loss: 1.0711\n",
      "G_total_loss: 2.7771\n",
      "├─ G_global_loss: 0.8579\n",
      "└─ G_pixel_loss: 1.9192\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 3.1726\n",
      "├─ Enc_loss: 1.3368\n",
      "├─ Dec_loss: 1.8359\n",
      "G_total_loss: 1.7083\n",
      "├─ G_global_loss: 0.7001\n",
      "└─ G_pixel_loss: 1.0082\n",
      "\n",
      "Epoch [11/70] - Epoch Time: 15.13s - Total Time: 00:02:42 \n",
      "D_total_loss: 2.8507\n",
      "├─ Enc_loss: 1.2887\n",
      "├─ Dec_loss: 1.5620\n",
      "G_total_loss: 1.8292\n",
      "├─ G_global_loss: 0.7171\n",
      "└─ G_pixel_loss: 1.1121\n",
      "\n",
      "Latest JSD Score: 0.6379\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.4018\n",
      "├─ Enc_loss: 1.1754\n",
      "├─ Dec_loss: 1.2263\n",
      "G_total_loss: 2.0792\n",
      "├─ G_global_loss: 0.8443\n",
      "└─ G_pixel_loss: 1.2349\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.1740\n",
      "├─ Enc_loss: 1.1569\n",
      "├─ Dec_loss: 1.0171\n",
      "G_total_loss: 1.9162\n",
      "├─ G_global_loss: 0.8225\n",
      "└─ G_pixel_loss: 1.0937\n",
      "\n",
      "Epoch [12/70] - Epoch Time: 15.22s - Total Time: 00:02:57 \n",
      "D_total_loss: 2.3515\n",
      "├─ Enc_loss: 1.1774\n",
      "├─ Dec_loss: 1.1741\n",
      "G_total_loss: 2.2216\n",
      "├─ G_global_loss: 0.8205\n",
      "└─ G_pixel_loss: 1.4011\n",
      "\n",
      "Latest JSD Score: 0.5321\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.3327\n",
      "├─ Enc_loss: 1.1535\n",
      "├─ Dec_loss: 1.1793\n",
      "G_total_loss: 2.3770\n",
      "├─ G_global_loss: 0.8493\n",
      "└─ G_pixel_loss: 1.5277\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.8024\n",
      "├─ Enc_loss: 1.3218\n",
      "├─ Dec_loss: 1.4806\n",
      "G_total_loss: 1.6965\n",
      "├─ G_global_loss: 0.6922\n",
      "└─ G_pixel_loss: 1.0043\n",
      "\n",
      "Epoch [13/70] - Epoch Time: 15.47s - Total Time: 00:03:12 \n",
      "D_total_loss: 2.6674\n",
      "├─ Enc_loss: 1.2705\n",
      "├─ Dec_loss: 1.3970\n",
      "G_total_loss: 1.8879\n",
      "├─ G_global_loss: 0.7360\n",
      "└─ G_pixel_loss: 1.1519\n",
      "\n",
      "Latest JSD Score: 0.5398\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.4690\n",
      "├─ Enc_loss: 1.1994\n",
      "├─ Dec_loss: 1.2696\n",
      "G_total_loss: 2.1369\n",
      "├─ G_global_loss: 0.8572\n",
      "└─ G_pixel_loss: 1.2797\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.4161\n",
      "├─ Enc_loss: 1.2047\n",
      "├─ Dec_loss: 1.2114\n",
      "G_total_loss: 2.0886\n",
      "├─ G_global_loss: 0.8151\n",
      "└─ G_pixel_loss: 1.2735\n",
      "\n",
      "Epoch [14/70] - Epoch Time: 15.40s - Total Time: 00:03:28 \n",
      "D_total_loss: 2.3556\n",
      "├─ Enc_loss: 1.1872\n",
      "├─ Dec_loss: 1.1684\n",
      "G_total_loss: 2.2370\n",
      "├─ G_global_loss: 0.8187\n",
      "└─ G_pixel_loss: 1.4183\n",
      "\n",
      "Latest JSD Score: 0.5243\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.1179\n",
      "├─ Enc_loss: 1.1406\n",
      "├─ Dec_loss: 0.9773\n",
      "G_total_loss: 2.4239\n",
      "├─ G_global_loss: 0.8440\n",
      "└─ G_pixel_loss: 1.5799\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.8633\n",
      "├─ Enc_loss: 1.3470\n",
      "├─ Dec_loss: 1.5162\n",
      "G_total_loss: 1.7674\n",
      "├─ G_global_loss: 0.7129\n",
      "└─ G_pixel_loss: 1.0546\n",
      "\n",
      "Epoch [15/70] - Epoch Time: 14.92s - Total Time: 00:03:43 \n",
      "D_total_loss: 2.6597\n",
      "├─ Enc_loss: 1.2836\n",
      "├─ Dec_loss: 1.3762\n",
      "G_total_loss: 1.8687\n",
      "├─ G_global_loss: 0.7307\n",
      "└─ G_pixel_loss: 1.1379\n",
      "\n",
      "Latest JSD Score: 0.5263\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.3791\n",
      "├─ Enc_loss: 1.1711\n",
      "├─ Dec_loss: 1.2080\n",
      "G_total_loss: 2.6411\n",
      "├─ G_global_loss: 0.8645\n",
      "└─ G_pixel_loss: 1.7765\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.1130\n",
      "├─ Enc_loss: 1.1835\n",
      "├─ Dec_loss: 0.9295\n",
      "G_total_loss: 2.3324\n",
      "├─ G_global_loss: 0.8187\n",
      "└─ G_pixel_loss: 1.5137\n",
      "\n",
      "Epoch [16/70] - Epoch Time: 14.85s - Total Time: 00:03:57 \n",
      "D_total_loss: 2.3377\n",
      "├─ Enc_loss: 1.1910\n",
      "├─ Dec_loss: 1.1467\n",
      "G_total_loss: 2.1696\n",
      "├─ G_global_loss: 0.8184\n",
      "└─ G_pixel_loss: 1.3512\n",
      "\n",
      "Latest JSD Score: 0.5146\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.3178\n",
      "├─ Enc_loss: 1.1706\n",
      "├─ Dec_loss: 1.1472\n",
      "G_total_loss: 2.5139\n",
      "├─ G_global_loss: 0.8509\n",
      "└─ G_pixel_loss: 1.6630\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.4586\n",
      "├─ Enc_loss: 1.3265\n",
      "├─ Dec_loss: 1.1322\n",
      "G_total_loss: 2.0901\n",
      "├─ G_global_loss: 0.7251\n",
      "└─ G_pixel_loss: 1.3650\n",
      "\n",
      "Epoch [17/70] - Epoch Time: 14.95s - Total Time: 00:04:12 \n",
      "D_total_loss: 2.5972\n",
      "├─ Enc_loss: 1.2715\n",
      "├─ Dec_loss: 1.3257\n",
      "G_total_loss: 1.9701\n",
      "├─ G_global_loss: 0.7410\n",
      "└─ G_pixel_loss: 1.2291\n",
      "\n",
      "Latest JSD Score: 0.5419\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.2592\n",
      "├─ Enc_loss: 1.2164\n",
      "├─ Dec_loss: 1.0428\n",
      "G_total_loss: 2.1870\n",
      "├─ G_global_loss: 0.8422\n",
      "└─ G_pixel_loss: 1.3447\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.4407\n",
      "├─ Enc_loss: 1.1818\n",
      "├─ Dec_loss: 1.2588\n",
      "G_total_loss: 2.6358\n",
      "├─ G_global_loss: 0.8274\n",
      "└─ G_pixel_loss: 1.8085\n",
      "\n",
      "Epoch [18/70] - Epoch Time: 14.88s - Total Time: 00:04:27 \n",
      "D_total_loss: 2.2754\n",
      "├─ Enc_loss: 1.1876\n",
      "├─ Dec_loss: 1.0877\n",
      "G_total_loss: 2.2058\n",
      "├─ G_global_loss: 0.8216\n",
      "└─ G_pixel_loss: 1.3842\n",
      "\n",
      "Latest JSD Score: 0.5197\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.3381\n",
      "├─ Enc_loss: 1.1157\n",
      "├─ Dec_loss: 1.2224\n",
      "G_total_loss: 2.4827\n",
      "├─ G_global_loss: 0.8457\n",
      "└─ G_pixel_loss: 1.6370\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.7534\n",
      "├─ Enc_loss: 1.3036\n",
      "├─ Dec_loss: 1.4498\n",
      "G_total_loss: 2.1184\n",
      "├─ G_global_loss: 0.7257\n",
      "└─ G_pixel_loss: 1.3927\n",
      "\n",
      "Epoch [19/70] - Epoch Time: 14.76s - Total Time: 00:04:42 \n",
      "D_total_loss: 2.5769\n",
      "├─ Enc_loss: 1.2765\n",
      "├─ Dec_loss: 1.3004\n",
      "G_total_loss: 1.9942\n",
      "├─ G_global_loss: 0.7344\n",
      "└─ G_pixel_loss: 1.2598\n",
      "\n",
      "Latest JSD Score: 0.5658\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 2.3047\n",
      "├─ Enc_loss: 1.2068\n",
      "├─ Dec_loss: 1.0979\n",
      "G_total_loss: 2.5346\n",
      "├─ G_global_loss: 0.8885\n",
      "└─ G_pixel_loss: 1.6461\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.1148\n",
      "├─ Enc_loss: 1.1596\n",
      "├─ Dec_loss: 0.9552\n",
      "G_total_loss: 2.0051\n",
      "├─ G_global_loss: 0.8429\n",
      "└─ G_pixel_loss: 1.1622\n",
      "\n",
      "Epoch [20/70] - Epoch Time: 15.49s - Total Time: 00:04:58 \n",
      "D_total_loss: 2.1861\n",
      "├─ Enc_loss: 1.1808\n",
      "├─ Dec_loss: 1.0053\n",
      "G_total_loss: 2.2290\n",
      "├─ G_global_loss: 0.8346\n",
      "└─ G_pixel_loss: 1.3944\n",
      "\n",
      "Latest JSD Score: 0.4995\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 1.8753\n",
      "├─ Enc_loss: 1.1112\n",
      "├─ Dec_loss: 0.7641\n",
      "G_total_loss: 2.8043\n",
      "├─ G_global_loss: 0.8582\n",
      "└─ G_pixel_loss: 1.9461\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 2.5633\n",
      "├─ Enc_loss: 1.2008\n",
      "├─ Dec_loss: 1.3625\n",
      "G_total_loss: 2.1375\n",
      "├─ G_global_loss: 0.7714\n",
      "└─ G_pixel_loss: 1.3661\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m iteration_losses, total_iterations, jsd_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#fred_history\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plot_metrics(\n\u001b[0;32m      5\u001b[0m     iteration_losses\u001b[38;5;241m=\u001b[39miteration_losses,\n\u001b[0;32m      6\u001b[0m     jsd_history\u001b[38;5;241m=\u001b[39mjsd_history,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     dataloader_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[0;32m     11\u001b[0m )\n",
      "Cell \u001b[1;32mIn[11], line 187\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(generator, discriminator, dataloader, num_epochs, d_step, g_step)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Train discriminator\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_step):\n\u001b[1;32m--> 187\u001b[0m     d_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreal_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreal_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mfake_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfake_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# Sum up d_losses\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m d_losses:\n",
      "Cell \u001b[1;32mIn[11], line 54\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(discriminator, real_sequences, fake_sequences, optimizer)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 54\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Add gradient clipping here\u001b[39;00m\n\u001b[0;32m     57\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(generator\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kotsgeo\\AppData\\Local\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kotsgeo\\AppData\\Local\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kotsgeo\\AppData\\Local\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iteration_losses, total_iterations, jsd_history = train(generator, discriminator, dataloader,\n",
    "                                                       num_epochs=70, d_step=1, g_step=2) #fred_history\n",
    "\n",
    "plot_metrics(\n",
    "    iteration_losses=iteration_losses,\n",
    "    jsd_history=jsd_history,\n",
    "    # fred_history=fred_history,\n",
    "    total_iterations=total_iterations,\n",
    "    num_epochs=70,\n",
    "    dataloader_size=len(dataloader)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
