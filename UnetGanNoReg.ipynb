{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566e8a79-9171-493d-b782-80594774040b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.functional import gumbel_softmax\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b44246-4e72-47e4-93b8-dd58b842130c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use absolute path\n",
    "file_path = r\"C:\\Users\\kotsgeo\\Documents\\GANs\\AMPdata.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6b2c0ee-b385-4d91-aaae-8f0c5a0b7dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2600 sequences\n",
      "First sequence shape: torch.Size([156, 5])\n",
      "\n",
      "Mapping: {'P': 0, 'A': 1, 'T': 2, 'G': 3, 'C': 4}\n",
      "\n",
      "Number of sequences: 2600\n",
      "Batch shape: torch.Size([64, 156, 5])\n",
      "\n",
      "Sample from batch (showing where P padding is):\n",
      "tensor([1, 2, 3, 2, 4, 2, 2, 2, 3, 3])\n",
      "Successfully loaded 2600 sequences\n"
     ]
    }
   ],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, file_path, seq_length=156):\n",
    "        self.seq_length = seq_length\n",
    "        self.sequences = []\n",
    "        self.char_to_idx = {'P': 0, 'A': 1, 'T': 2, 'G': 3, 'C': 4}\n",
    "        \n",
    "        # Handle both absolute and relative paths\n",
    "        if not os.path.isabs(file_path):\n",
    "            file_path = os.path.join(os.getcwd(), file_path)\n",
    "            \n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    seq = line.strip().split('\\t')[0]\n",
    "                    if len(seq) < seq_length:\n",
    "                        seq = seq + 'P' * (seq_length - len(seq))\n",
    "                    seq = seq[:seq_length]\n",
    "                    self.sequences.append(seq)\n",
    "            print(f\"Successfully loaded {len(self.sequences)} sequences\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Data file not found at: {file_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        one_hot = torch.zeros(self.seq_length, len(self.char_to_idx))\n",
    "        for i, char in enumerate(seq[:self.seq_length]):\n",
    "            if char in self.char_to_idx:\n",
    "                one_hot[i][self.char_to_idx[char]] = 1\n",
    "            else:\n",
    "                one_hot[i][self.char_to_idx['P']] = 1\n",
    "        return one_hot\n",
    "\n",
    "# Test the dataset\n",
    "if __name__ == \"__main__\":\n",
    "    # You can use either relative or absolute path\n",
    "    # Option 1: Relative path\n",
    "    dataset = SequenceDataset(\"AMPdata.txt\")\n",
    "    \n",
    "    # Option 2: Absolute path\n",
    "    # dataset = SequenceDataset(\"/path/to/your/AMPdata.txt\")\n",
    "\n",
    "    # Print first sequence\n",
    "    first_seq = dataset[0]\n",
    "    print(\"First sequence shape:\", first_seq.shape)\n",
    "    print(\"\\nMapping:\", dataset.char_to_idx)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "\n",
    "    print('\\nNumber of sequences:', len(dataloader.dataset))\n",
    "    \n",
    "    # Check a batch\n",
    "    for batch in dataloader:\n",
    "        print(\"Batch shape:\", batch.shape)\n",
    "        print(\"\\nSample from batch (showing where P padding is):\")\n",
    "        print(torch.argmax(batch[0], dim=1)[:10])  \n",
    "        break\n",
    "dataset = SequenceDataset(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43347fb8-81c8-4405-9481-0b9b313ed5a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, hidden):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.hidden_channels = hidden // 4\n",
    "        \n",
    "        # Down projection\n",
    "        self.down_proj = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(hidden, self.hidden_channels, 1, padding=0)\n",
    "        )\n",
    "        \n",
    "        # Main processing\n",
    "        self.main_proc = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.hidden_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(self.hidden_channels, self.hidden_channels, 5, padding=2),\n",
    "            \n",
    "            nn.BatchNorm1d(self.hidden_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(self.hidden_channels, self.hidden_channels, 5, padding=2)\n",
    "        )\n",
    "        \n",
    "        # Up projection\n",
    "        self.up_proj = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.hidden_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(self.hidden_channels, hidden, 1, padding=0),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.down_proj(x)\n",
    "        h = self.main_proc(h)\n",
    "        h = self.up_proj(h)\n",
    "        return x + h\n",
    "    \n",
    "class ImprovedGenerator(nn.Module):\n",
    "    def __init__(self, n_chars, seq_len, batch_size, hidden):\n",
    "        super(ImprovedGenerator, self).__init__()\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.n_chars = n_chars\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden = hidden\n",
    "        \n",
    "        # Start from length 5\n",
    "        self.bottom_length = 5\n",
    "        \n",
    "        # Initial projection\n",
    "        self.linear = nn.Linear(128, hidden*16*self.bottom_length)\n",
    "        \n",
    "        # Progressive upsampling blocks\n",
    "        # 5 -> 10 -> 20 -> 40 -> 80 -> 156\n",
    "        self.upscale_factors = [2, 2, 2, 2, 1.95]  # Last step adjusts to reach exactly 156\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            # First block: maintain channels\n",
    "            nn.Sequential(\n",
    "                ResBlock(hidden*16),              \n",
    "                nn.Upsample(scale_factor=self.upscale_factors[0], mode='nearest')  # 5 -> 10\n",
    "            ),\n",
    "            \n",
    "            # Second block: 16ch → 8ch\n",
    "            nn.Sequential(\n",
    "                ResBlock(hidden*16),          \n",
    "                nn.Conv1d(hidden*16, hidden*8, 1),\n",
    "                nn.Upsample(scale_factor=self.upscale_factors[1])  # 10 -> 20\n",
    "            ),\n",
    "            \n",
    "            # Third block: 8ch → 4ch\n",
    "            nn.Sequential(\n",
    "                ResBlock(hidden*8),           \n",
    "                nn.Conv1d(hidden*8, hidden*4, 1),\n",
    "                nn.Upsample(scale_factor=self.upscale_factors[2])  # 20 -> 40\n",
    "            ),\n",
    "            \n",
    "            # Fourth block: 4ch → 2ch\n",
    "            nn.Sequential(\n",
    "                ResBlock(hidden*4),           \n",
    "                nn.Conv1d(hidden*4, hidden*2, 1),\n",
    "                nn.Upsample(scale_factor=self.upscale_factors[3])  # 40 -> 80\n",
    "            ),\n",
    "            \n",
    "            # Fifth block: 2ch → ch\n",
    "            nn.Sequential(\n",
    "                ResBlock(hidden*2),           \n",
    "                nn.Conv1d(hidden*2, hidden, 1),\n",
    "                nn.Upsample(scale_factor=self.upscale_factors[4])  # 80 -> 156\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Final layers\n",
    "        self.final = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(hidden, n_chars, 3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, noise):\n",
    "        # Initial projection and reshape\n",
    "        output = self.linear(noise)  # [64, 5120]\n",
    "        output = output.view(-1, self.hidden*16, self.bottom_length)  # [64, 1024, 5]\n",
    "        \n",
    "        # Through progressive blocks\n",
    "        for block in self.blocks:\n",
    "            output = block(output)\n",
    "            \n",
    "        # Final processing\n",
    "        output = self.final(output)  # [64, 5, 156]\n",
    "        \n",
    "        # Prepare for Gumbel\n",
    "        output = output.transpose(1, 2)  # [64, 156, 5]\n",
    "        shape = output.size()\n",
    "        output = output.contiguous()\n",
    "        output = output.view(-1, self.n_chars)  # [9984, 5]\n",
    "        output = gumbel_softmax(output, 0.5)\n",
    "        \n",
    "        return output.view(shape)  # Back to [64, 156, 5]\n",
    "\n",
    "class UNetDiscriminator(nn.Module):\n",
    "    def __init__(self, n_chars, seq_len, batch_size, hidden):\n",
    "        super(UNetDiscriminator, self).__init__()\n",
    "\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, (nn.Conv1d, nn.Linear)):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.n_chars = n_chars\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden = hidden\n",
    "\n",
    "        # Initial projection\n",
    "        self.initial_conv = nn.Conv1d(n_chars, hidden, 1)  # ch\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            ResBlock(hidden),              # 64\n",
    "            nn.Conv1d(hidden, hidden*2, 1),# -> 128\n",
    "            nn.AvgPool1d(2)               # 156 -> 78\n",
    "        )\n",
    "\n",
    "        self.enc2 = nn.Sequential(\n",
    "            ResBlock(hidden*2),           # 128\n",
    "            nn.Conv1d(hidden*2, hidden*4, 1),# -> 256\n",
    "            nn.AvgPool1d(2)               # 78 -> 39\n",
    "        )\n",
    "\n",
    "        self.enc3 = nn.Sequential(\n",
    "            ResBlock(hidden*4),           # 256\n",
    "            nn.Conv1d(hidden*4, hidden*8, 1),# -> 512\n",
    "            nn.AvgPool1d(2)               # 39 -> 19\n",
    "        )\n",
    "\n",
    "        self.enc4 = nn.Sequential(\n",
    "            ResBlock(hidden*8),           # 512\n",
    "            nn.Conv1d(hidden*8, hidden*16, 1),# -> 1024\n",
    "            nn.AvgPool1d(2)               # 19 -> 9\n",
    "        )\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ResBlock(hidden*16)  # 1024\n",
    "\n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            ResBlock(hidden*16),          # 1024\n",
    "            nn.Conv1d(hidden*16, hidden*8, 1),# -> 512\n",
    "            nn.Upsample(scale_factor=2)    # 9 -> 18\n",
    "        )\n",
    "        self.dec1_res = ResBlock(hidden*16)  # After concat (512+512=1024)\n",
    "\n",
    "        self.dec2 = nn.Sequential(\n",
    "            ResBlock(hidden*16),          # 1024\n",
    "            nn.Conv1d(hidden*16, hidden*8, 1),# -> 512\n",
    "            nn.Upsample(scale_factor=2)    # 19 -> 38\n",
    "        )\n",
    "        self.dec2_conv = nn.Conv1d(hidden*12, hidden*16, 1)  # 768->1024\n",
    "        self.dec2_res = ResBlock(hidden*16)  # After concat adjust\n",
    "\n",
    "        self.dec3 = nn.Sequential(\n",
    "            ResBlock(hidden*16),          # 1024\n",
    "            nn.Conv1d(hidden*16, hidden*4, 1),# -> 256\n",
    "            nn.Upsample(scale_factor=2)    # 39 -> 78\n",
    "        )\n",
    "        self.dec3_conv = nn.Conv1d(hidden*6, hidden*8, 1)  # 384->512\n",
    "        self.dec3_res = ResBlock(hidden*8)  # After concat adjust\n",
    "\n",
    "        self.dec4 = nn.Sequential(\n",
    "            ResBlock(hidden*8),           # 512\n",
    "            nn.Conv1d(hidden*8, hidden*2, 1),# -> 128\n",
    "            nn.Upsample(scale_factor=2)    # 78 -> 156\n",
    "        )\n",
    "        self.dec4_conv = nn.Conv1d(hidden*3, hidden*4, 1)  # 192->256\n",
    "        self.dec4_res = ResBlock(hidden*4)  # After concat adjust\n",
    "\n",
    "        # Global output (from bottleneck)\n",
    "        self.global_output = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),    \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(hidden*16, 1)      \n",
    "        )\n",
    "\n",
    "        # Pixel-wise output\n",
    "        self.pixel_output = nn.Conv1d(hidden*4, 1, 1)\n",
    "\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def pad_end(self, x, target_size):\n",
    "        \"\"\"Add padding at the end to reach target_size\"\"\"\n",
    "        diff = target_size - x.size(2)\n",
    "        if diff > 0:\n",
    "            return F.pad(x, (0, diff), mode='constant', value=0)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Input shape: (batch_size, seq_len, n_chars)\n",
    "        x = input.transpose(1, 2)  # (batch_size, n_chars, seq_len)\n",
    "\n",
    "        # Initial projection\n",
    "        x = self.initial_conv(x)  # [batch, hidden, 156]\n",
    "        residuals = [x]\n",
    "\n",
    "        # Encoder path\n",
    "        x = self.enc1(x)        # -> 78\n",
    "        residuals.append(x)\n",
    "        x = self.enc2(x)        # -> 39\n",
    "        residuals.append(x)\n",
    "        x = self.enc3(x)        # -> 19\n",
    "        residuals.append(x)\n",
    "        x = self.enc4(x)        # -> 9\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(x)  # Store bottleneck output\n",
    "\n",
    "        # Global output\n",
    "        global_output = self.global_output(bottleneck)\n",
    "        global_output = torch.sigmoid(global_output)\n",
    "\n",
    "        # Decoder path with padding and skip connections\n",
    "        x = self.dec1(bottleneck)  # 9 -> 18\n",
    "        x = self.pad_end(x, 19)    # pad to 19\n",
    "        x = torch.cat([x, residuals.pop()], dim=1)\n",
    "        x = self.dec1_res(x)\n",
    "\n",
    "        x = self.dec2(x)           # 19 -> 38\n",
    "        x = self.pad_end(x, 39)    # pad to 39\n",
    "        x = torch.cat([x, residuals.pop()], dim=1)\n",
    "        x = self.dec2_conv(x)      # Adjust channels\n",
    "        x = self.dec2_res(x)\n",
    "\n",
    "        x = self.dec3(x)           # 39 -> 78\n",
    "        x = torch.cat([x, residuals.pop()], dim=1)\n",
    "        x = self.dec3_conv(x)      # Adjust channels\n",
    "        x = self.dec3_res(x)\n",
    "\n",
    "        x = self.dec4(x)           # 78 -> 156\n",
    "        x = torch.cat([x, residuals.pop()], dim=1)\n",
    "        x = self.dec4_conv(x)      # Adjust channels\n",
    "        x = self.dec4_res(x)\n",
    "\n",
    "        # Pixel-wise output\n",
    "        pixel_output = self.pixel_output(x)\n",
    "        pixel_output = torch.sigmoid(pixel_output)\n",
    "        pixel_output = pixel_output.transpose(1, 2)\n",
    "\n",
    "        return global_output, pixel_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b4e9c9-a298-46c0-ba1a-88ed237be9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4219bc52-ae1f-481c-8fc6-a10e17aeea7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def sample_and_analyze(generator, num_samples=batch_size, epoch=0):\n",
    "    # Decode function\n",
    "    inv_charmap = {0:'P', 1:'A', 2:'T', 3:'G', 4:'C'}\n",
    "\n",
    "    def decode_sequence(seq):\n",
    "        indices = np.argmax(seq, axis=1)\n",
    "        return ''.join([inv_charmap[idx] for idx in indices])\n",
    "\n",
    "    # Generate sequences\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(num_samples, 128).to(device)\n",
    "        generated_sequences = generator(noise)\n",
    "\n",
    "        # Convert to list of strings\n",
    "        decoded_seqs = [decode_sequence(seq.cpu().numpy()) for seq in generated_sequences]\n",
    "\n",
    "    generator.train()\n",
    "\n",
    "    return decoded_seqs\n",
    "\n",
    "def analyze_sequences(seqs):\n",
    "    # Remove padding sequences\n",
    "    clean_seqs = [seq.replace('P', '') for seq in seqs]\n",
    "\n",
    "    # Analysis metrics\n",
    "    analysis = {\n",
    "        'total_sequences': len(seqs),\n",
    "        'sequences_without_padding': len(clean_seqs),\n",
    "\n",
    "        # Start codon analysis\n",
    "        'starts_with_atg': sum(1 for seq in clean_seqs if seq.startswith('ATG')),\n",
    "\n",
    "        # Stop codon analysis\n",
    "        'ends_with_stop_codon': sum(1 for seq in clean_seqs\n",
    "                                    if seq.endswith('TAA') or\n",
    "                                       seq.endswith('TAG') or\n",
    "                                       seq.endswith('TGA')),\n",
    "\n",
    "        # Multiple of 3 analysis\n",
    "        'multiple_of_3': sum(1 for seq in clean_seqs if len(seq) % 3 == 0),\n",
    "\n",
    "        # All three conditions\n",
    "        'valid_orfs': sum(1 for seq in clean_seqs \n",
    "                         if seq.startswith('ATG') and\n",
    "                            (seq.endswith('TAA') or seq.endswith('TAG') or seq.endswith('TGA')) and\n",
    "                            len(seq) % 3 == 0),\n",
    "\n",
    "        # GC Content analysis\n",
    "        'gc_content': [],\n",
    "        'avg_gc_content': 0,\n",
    "        'min_gc_content': 0,\n",
    "        'max_gc_content': 0\n",
    "    }\n",
    "\n",
    "    # Calculate GC content for each sequence\n",
    "    for seq in clean_seqs:\n",
    "        # Count G and C\n",
    "        gc_count = seq.count('G') + seq.count('C')\n",
    "        # Calculate GC content percentage\n",
    "        gc_content = (gc_count / len(seq)) * 100 if len(seq) > 0 else 0\n",
    "        analysis['gc_content'].append(gc_content)\n",
    "\n",
    "    # Calculate GC content statistics\n",
    "    if analysis['gc_content']:\n",
    "        analysis['avg_gc_content'] = np.mean(analysis['gc_content'])\n",
    "        analysis['min_gc_content'] = np.min(analysis['gc_content'])\n",
    "        analysis['max_gc_content'] = np.max(analysis['gc_content'])\n",
    "\n",
    "    # Calculate percentages\n",
    "    analysis['start_codon_percentage'] = (analysis['starts_with_atg'] / len(clean_seqs)) * 100 if clean_seqs else 0\n",
    "    analysis['stop_codon_percentage'] = (analysis['ends_with_stop_codon'] / len(clean_seqs)) * 100 if clean_seqs else 0\n",
    "    analysis['multiple_of_3_percentage'] = (analysis['multiple_of_3'] / len(clean_seqs)) * 100 if clean_seqs else 0\n",
    "    analysis['valid_orfs_percentage'] = (analysis['valid_orfs'] / len(clean_seqs)) * 100 if clean_seqs else 0\n",
    "\n",
    "    return analysis\n",
    "\n",
    "def save_analysis(generated_seqs, epoch):\n",
    "    # Perform analysis\n",
    "    seq_properties = analyze_sequences(generated_seqs)\n",
    "\n",
    "    # Create filename\n",
    "    filename = os.path.join(results_dir, f'analysis_epoch_{epoch}.txt')\n",
    "\n",
    "    # Write to file\n",
    "    with open(filename, 'w') as f:\n",
    "        # First, write all generated sequences\n",
    "        f.write(\"Generated Sequences:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        for i, seq in enumerate(generated_seqs, 1):\n",
    "            f.write(f\"Sequence {i}: {seq}\\n\")\n",
    "\n",
    "        # Then write analysis results\n",
    "        f.write(\"\\n\\nSequence Analysis:\\n\")\n",
    "        f.write(\"-\" * 20 + \"\\n\")\n",
    "        f.write(f\"Total Sequences: {seq_properties['total_sequences']}\\n\")\n",
    "        f.write(f\"Sequences without padding: {seq_properties['sequences_without_padding']}\\n\")\n",
    "\n",
    "        f.write(\"\\nStart Codon Analysis:\\n\")\n",
    "        f.write(f\"Starts with ATG: {seq_properties['starts_with_atg']} \"\n",
    "                f\"({seq_properties['start_codon_percentage']:.2f}%)\\n\")\n",
    "\n",
    "        f.write(\"\\nStop Codon Analysis:\\n\")\n",
    "        f.write(f\"Ends with stop codon: {seq_properties['ends_with_stop_codon']} \"\n",
    "                f\"({seq_properties['stop_codon_percentage']:.2f}%)\\n\")\n",
    "\n",
    "        f.write(\"\\nSequence Length Analysis:\\n\")\n",
    "        f.write(f\"Sequences multiple of 3: {seq_properties['multiple_of_3']} \"\n",
    "                f\"({seq_properties['multiple_of_3_percentage']:.2f}%)\\n\")\n",
    "\n",
    "        f.write(\"\\nValid ORFs Analysis:\\n\")\n",
    "        f.write(f\"Sequences with all conditions (start, stop, multiple of 3): {seq_properties['valid_orfs']} \"\n",
    "                f\"({seq_properties['valid_orfs_percentage']:.2f}%)\\n\")\n",
    "\n",
    "        f.write(\"\\nGC Content Analysis:\\n\")\n",
    "        f.write(f\"Average GC Content: {seq_properties['avg_gc_content']:.2f}%\\n\")\n",
    "        f.write(f\"Minimum GC Content: {seq_properties['min_gc_content']:.2f}%\\n\")\n",
    "        f.write(f\"Maximum GC Content: {seq_properties['max_gc_content']:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7497e350-fb85-43f3-84d7-6e1a88aa31e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define results directory with absolute path\n",
    "results_dir = r\"C:\\Users\\kotsgeo\\Documents\\GANs\\GANs\\ResultsNoReg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "473d4110-a8b1-46bd-9b97-d89af5f1bcb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_vector_boundingbox(length, lam):\n",
    "    # Calculate box length based on lambda\n",
    "    r = np.sqrt(1. - lam)\n",
    "    box_length = int(length * r)\n",
    "    \n",
    "    # Randomly select start position\n",
    "    start = np.random.randint(0, length - box_length + 1)\n",
    "    \n",
    "    # Calculate end position\n",
    "    end = start + box_length\n",
    "    \n",
    "    return start, end\n",
    "\n",
    "def create_cutmix_mask(real_sequences, lam=None):\n",
    "    batch_size, seq_len = real_sequences.size(0), real_sequences.size(1)\n",
    "    \n",
    "    # Create mask for each sequence in the batch\n",
    "    masks = []\n",
    "    for i in range(batch_size):\n",
    "        # Generate lambda if not provided\n",
    "        if lam is None:\n",
    "            lam = np.random.beta(1, 1)\n",
    "        \n",
    "        # Create mask for this sequence\n",
    "        mask = torch.ones(seq_len)\n",
    "        start, end = random_vector_boundingbox(seq_len, lam)\n",
    "        mask[start:end] = 0\n",
    "        \n",
    "        # 50% chance to flip mask\n",
    "        if torch.rand(1) > 0.5:\n",
    "            mask = 1 - mask\n",
    "        \n",
    "        masks.append(mask)\n",
    "    \n",
    "    # Stack masks and add channel dimension\n",
    "    return torch.stack(masks).unsqueeze(-1).to(real_sequences.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10236651-6708-4e6b-9d94-9919ad2a7f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# 1. Convert one-hot or Gumbel-softmax output to DNA string\n",
    "def tensor_to_dna(seq_tensor):\n",
    "    idx_to_base = ['P', 'A', 'T', 'G', 'C']\n",
    "    indices = torch.argmax(seq_tensor, dim=-1)\n",
    "    return ''.join([idx_to_base[i] for i in indices.tolist() if idx_to_base[i] != 'P'])\n",
    "\n",
    "# 2. Get k-mer distribution\n",
    "def get_kmer_distribution(seqs, k=6):\n",
    "    all_kmers = [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "    kmer_counts = Counter()\n",
    "    for seq in seqs:\n",
    "        for i in range(len(seq) - k + 1):\n",
    "            kmer = seq[i:i+k]\n",
    "            kmer_counts[kmer] += 1\n",
    "    total = sum(kmer_counts.values())\n",
    "    freq = np.array([kmer_counts[kmer] for kmer in all_kmers], dtype=np.float32)\n",
    "    return freq / (total + 1e-8)\n",
    "\n",
    "# 3. Evaluate per epoch\n",
    "def evaluate_kmer_jsd(real_batch, gen_batch, k=6):\n",
    "    real_seqs = [tensor_to_dna(seq) for seq in real_batch]  # shape: (batch_size, seq_len, 4)\n",
    "    gen_seqs  = [tensor_to_dna(seq) for seq in gen_batch]\n",
    "\n",
    "    real_dist = get_kmer_distribution(real_seqs, k=k)\n",
    "    gen_dist  = get_kmer_distribution(gen_seqs, k=k)\n",
    "\n",
    "    jsd = jensenshannon(real_dist, gen_dist)\n",
    "    return jsd\n",
    "\n",
    "def evaluate_multiple_batches(generator, dataloader, num_batches=5, k=6):\n",
    "    generator.eval()  # Set to evaluation mode\n",
    "    total_jsd = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            # Get real batch\n",
    "            try:\n",
    "                real_batch = next(iter(dataloader)).to(device)\n",
    "            except StopIteration:\n",
    "                dataloader_iter = iter(dataloader)\n",
    "                real_batch = next(dataloader_iter).to(device)\n",
    "            \n",
    "            # Generate fake batch\n",
    "            noise = torch.randn(real_batch.size(0), 128).to(device)\n",
    "            fake_batch = generator(noise)\n",
    "            \n",
    "            # Calculate JSD for this batch\n",
    "            batch_jsd = evaluate_kmer_jsd(real_batch, fake_batch, k=k)\n",
    "            total_jsd += batch_jsd\n",
    "    \n",
    "    # Calculate average JSD\n",
    "    avg_jsd = total_jsd / num_batches\n",
    "    return avg_jsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24f012-a1f6-40a5-ac2b-7a60ba97aef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_chars = 5\n",
    "seq_len = 156\n",
    "batch_size = 64\n",
    "hidden1 = 64\n",
    "hidden2 = 128\n",
    "num_epochs = 70\n",
    "\n",
    "generator = ImprovedGenerator(n_chars, seq_len, batch_size, hidden1).to(device)\n",
    "discriminator = UNetDiscriminator(n_chars, seq_len, batch_size, hidden2).to(device)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.00001, betas=(0.9, 0.999))\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "\n",
    "d_scheduler = CosineAnnealingLR(d_optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "g_scheduler = CosineAnnealingLR(g_optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = int(seconds % 60)\n",
    "    return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "\n",
    "def train_step(discriminator, real_sequences, fake_sequences, mixed_sequences, mask, optimizer):\n",
    "    batch_size = real_sequences.size(0)\n",
    "\n",
    "    # Get predictions for real and fake\n",
    "    real_global, real_pixel = discriminator(real_sequences)\n",
    "    fake_global, fake_pixel = discriminator(fake_sequences)\n",
    "    mixed_global, mixed_pixel = discriminator(mixed_sequences)\n",
    "    \n",
    "    # Calculate losses\n",
    "    enc_loss = -torch.mean(torch.log(real_global + 1e-8) +\n",
    "                      torch.log(1 - fake_global + 1e-8) +\n",
    "                      torch.log(1 - mixed_global + 1e-8))\n",
    "\n",
    "    dec_loss = -torch.mean(\n",
    "        torch.log(real_pixel + 1e-8) +\n",
    "        torch.log(1 - fake_pixel + 1e-8) +\n",
    "        torch.log(mixed_pixel * mask + 1e-8) +\n",
    "        torch.log(1 - mixed_pixel * (1 - mask) + 1e-8)\n",
    "    )\n",
    "\n",
    "    # Total discriminator loss\n",
    "    total_loss = enc_loss + dec_loss \n",
    "\n",
    "    # Update weights\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward(retain_graph=True)\n",
    "\n",
    "    # Add gradient clipping here\n",
    "    torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return {\n",
    "        'enc_loss': enc_loss.item(),\n",
    "        'dec_loss': dec_loss.item(),\n",
    "        'total_loss': total_loss.item()\n",
    "    }\n",
    "\n",
    "def generator_step(generator, discriminator, batch_size, optimizer):\n",
    "    # Generate fake sequences\n",
    "    noise = torch.randn(batch_size, 128).to(device)\n",
    "    fake_sequences = generator(noise)\n",
    "\n",
    "    # Get discriminator predictions\n",
    "    fake_global, fake_pixel = discriminator(fake_sequences)\n",
    "\n",
    "    # Generator loss\n",
    "\n",
    "    # Diversity penalty\n",
    "    diversity_loss = -torch.mean(torch.abs(torch.diff(fake_sequences, dim=1)))\n",
    "\n",
    "    # Global loss\n",
    "    g_global_loss = -torch.mean(torch.log(fake_global + 1e-8))\n",
    "\n",
    "    # Pixel-wise loss\n",
    "    g_pixel_loss = -torch.mean(torch.log(fake_pixel + 1e-8))\n",
    "\n",
    "    # Total generator loss\n",
    "    g_loss = g_global_loss + g_pixel_loss #+ 1*diversity_loss\n",
    "\n",
    "    # Update weights\n",
    "    optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "\n",
    "    # Add gradient clipping here\n",
    "    torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    return {\n",
    "        'g_global_loss': g_global_loss.item(),\n",
    "        'g_pixel_loss': g_pixel_loss.item(),\n",
    "        'g_total_loss': g_loss.item()\n",
    "    }\n",
    "\n",
    "def train(generator, discriminator, dataloader, num_epochs, d_step, g_step=1):\n",
    "    # Initialize JSD score\n",
    "    jsd_history = []\n",
    "    \n",
    "    # Initialize lists to store losses for each iteration\n",
    "    iteration_losses = {\n",
    "        # Discriminator Losses\n",
    "        'total_d_loss': [],\n",
    "        'enc_loss': [],\n",
    "        'dec_loss': [],\n",
    "\n",
    "        # Generator Losses\n",
    "        'total_g_loss': [],\n",
    "        'g_global_loss': [],\n",
    "        'g_pixel_loss': [],\n",
    "    }\n",
    "\n",
    "    total_iterations = 0\n",
    "\n",
    "    # Add time tracking\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Initialize running averages\n",
    "        running_losses = {\n",
    "            'total_d_loss': 0,\n",
    "            'enc_loss': 0,\n",
    "            'dec_loss': 0,\n",
    "            'total_g_loss': 0,\n",
    "            'g_global_loss': 0,\n",
    "            'g_pixel_loss': 0\n",
    "        }\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        for batch_idx, real_sequences in enumerate(dataloader):\n",
    "            total_iterations += 1\n",
    "\n",
    "            real_sequences = real_sequences.to(device)\n",
    "            batch_size = real_sequences.size(0)\n",
    "\n",
    "            # Generate fake sequences\n",
    "            noise = torch.randn(batch_size, 128).to(device)\n",
    "            fake_sequences = generator(noise)\n",
    "            \n",
    "            # Create CutMix mask and mixed sequences\n",
    "            mask = create_cutmix_mask(real_sequences, lam=0.5)\n",
    "            mixed_sequences = mask * real_sequences + (1 - mask) * fake_sequences\n",
    "            \n",
    "            # Create CutMix mask\n",
    "            mask = create_cutmix_mask(real_sequences, lam = 0.5)\n",
    "            \n",
    "            # Mix sequences using CutMix approach\n",
    "            mixed_sequences = (\n",
    "                mask * real_sequences + \n",
    "                (1 - mask) * fake_sequences\n",
    "            )\n",
    "            \n",
    "            # Train discriminator multiple times\n",
    "            d_losses_sum = {\n",
    "                'total_loss': 0,\n",
    "                'enc_loss': 0,\n",
    "                'dec_loss': 0,\n",
    "                'cons_loss': 0\n",
    "            }\n",
    "\n",
    "            # Train discriminator\n",
    "            for _ in range(d_step):\n",
    "                d_losses = train_step(discriminator,\n",
    "                                    real_sequences=real_sequences,\n",
    "                                    fake_sequences=fake_sequences,\n",
    "                                    mixed_sequences=mixed_sequences,\n",
    "                                    mask=mask,\n",
    "                                    optimizer=d_optimizer)\n",
    "\n",
    "                # Sum up d_losses\n",
    "                for key in d_losses:\n",
    "                    d_losses_sum[key] += d_losses[key]\n",
    "            \n",
    "            # Average d_losses over d_steps\n",
    "            d_losses_avg = {k: v/d_step for k, v in d_losses_sum.items()}\n",
    "            \n",
    "            # Train generator multiple times\n",
    "            g_losses_sum = {\n",
    "                'g_total_loss': 0,\n",
    "                'g_global_loss': 0,\n",
    "                'g_pixel_loss': 0\n",
    "            }\n",
    "\n",
    "            # Train generator\n",
    "            for _ in range(g_step):\n",
    "                g_losses = generator_step(generator, discriminator, batch_size, g_optimizer)\n",
    "            \n",
    "                # Sum up g_losses\n",
    "                for key in g_losses:\n",
    "                    g_losses_sum[key] += g_losses[key]\n",
    "\n",
    "            # Average g_losses over g_steps\n",
    "            g_losses_avg = {k: v/g_step for k, v in g_losses_sum.items()}\n",
    "\n",
    "            # Store iteration losses (using averages)\n",
    "            iteration_losses['total_d_loss'].append(d_losses_avg['total_loss'])\n",
    "            iteration_losses['enc_loss'].append(d_losses_avg['enc_loss'])\n",
    "            iteration_losses['dec_loss'].append(d_losses_avg['dec_loss'])\n",
    "            iteration_losses['total_g_loss'].append(g_losses_avg['g_total_loss'])\n",
    "            iteration_losses['g_global_loss'].append(g_losses_avg['g_global_loss'])\n",
    "            iteration_losses['g_pixel_loss'].append(g_losses_avg['g_pixel_loss'])\n",
    "\n",
    "            # Update running averages\n",
    "            running_losses['total_d_loss'] += d_losses_avg['total_loss']\n",
    "            running_losses['enc_loss'] += d_losses_avg['enc_loss']\n",
    "            running_losses['dec_loss'] += d_losses_avg['dec_loss']\n",
    "            running_losses['total_g_loss'] += g_losses_avg['g_total_loss']\n",
    "            running_losses['g_global_loss'] += g_losses_avg['g_global_loss']\n",
    "            running_losses['g_pixel_loss'] += g_losses_avg['g_pixel_loss']\n",
    "\n",
    "            # Print batch progress\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f'Batch [{batch_idx+1}/{num_batches}]')\n",
    "                print(f'D_total_loss: {d_losses_avg[\"total_loss\"]:.4f}')\n",
    "                print(f'├─ Enc_loss: {d_losses_avg[\"enc_loss\"]:.4f}')\n",
    "                print(f'├─ Dec_loss: {d_losses_avg[\"dec_loss\"]:.4f}')\n",
    "                print(f'G_total_loss: {g_losses_avg[\"g_total_loss\"]:.4f}')\n",
    "                print(f'├─ G_global_loss: {g_losses_avg[\"g_global_loss\"]:.4f}')\n",
    "                print(f'└─ G_pixel_loss: {g_losses_avg[\"g_pixel_loss\"]:.4f}\\n')\n",
    "\n",
    "        # Calculate JSD every N epochs (e.g., 5)\n",
    "        if epoch % 1 == 0:\n",
    "            current_jsd = evaluate_multiple_batches(generator, dataloader, num_batches=5)\n",
    "            jsd_history.append(current_jsd)\n",
    "        \n",
    "        # Calculate time for this epoch\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        # Calculate epoch averages\n",
    "        avg_losses = {k: v/num_batches for k, v in running_losses.items()}\n",
    "        \n",
    "        # Print epoch averages\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
    "            f'Epoch Time: {epoch_time:.2f}s - '\n",
    "            f'Total Time: {format_time(total_time)} ')\n",
    "        print(f'D_total_loss: {avg_losses[\"total_d_loss\"]:.4f}')\n",
    "        print(f'├─ Enc_loss: {avg_losses[\"enc_loss\"]:.4f}')\n",
    "        print(f'├─ Dec_loss: {avg_losses[\"dec_loss\"]:.4f}')\n",
    "        print(f'G_total_loss: {avg_losses[\"total_g_loss\"]:.4f}')\n",
    "        print(f'├─ G_global_loss: {avg_losses[\"g_global_loss\"]:.4f}')\n",
    "        print(f'└─ G_pixel_loss: {avg_losses[\"g_pixel_loss\"]:.4f}\\n')\n",
    "        print(f'Latest JSD Score: {current_jsd:.4f}')\n",
    "        print(50*\"-\")\n",
    "        \n",
    "        d_scheduler.step()\n",
    "        g_scheduler.step()\n",
    "        \n",
    "        if epoch % 2 == 0:\n",
    "            generated_seqs = sample_and_analyze(generator, num_samples=batch_size, epoch=epoch)\n",
    "            save_analysis(generated_seqs, epoch)\n",
    "\n",
    "    return iteration_losses, total_iterations, jsd_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef92079a-3cc2-4efd-8e62-24b7c369e5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics(iteration_losses, jsd_history, total_iterations, num_epochs, dataloader_size):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    # First subplot for iteration losses\n",
    "    plt.subplot(3, 1, 1)\n",
    "    iterations = range(total_iterations)\n",
    "    plt.plot(iterations, iteration_losses['total_d_loss'], label='D Loss', color='blue')\n",
    "    plt.plot(iterations, iteration_losses['total_g_loss'], label='G Loss', color='red')\n",
    "    plt.title('Generator and Discriminator Losses per Iteration')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Second subplot for epoch losses\n",
    "    plt.subplot(3, 1, 2)\n",
    "    # Calculate average loss per epoch\n",
    "    epochs = range(num_epochs)\n",
    "    d_losses_per_epoch = []\n",
    "    g_losses_per_epoch = []\n",
    "    \n",
    "    for epoch in epochs:\n",
    "        start_idx = epoch * dataloader_size\n",
    "        end_idx = (epoch + 1) * dataloader_size\n",
    "        \n",
    "        d_epoch_loss = np.mean(iteration_losses['total_d_loss'][start_idx:end_idx])\n",
    "        g_epoch_loss = np.mean(iteration_losses['total_g_loss'][start_idx:end_idx])\n",
    "        \n",
    "        d_losses_per_epoch.append(d_epoch_loss)\n",
    "        g_losses_per_epoch.append(g_epoch_loss)\n",
    "    \n",
    "    plt.plot(epochs, d_losses_per_epoch, label='D Loss', color='blue')\n",
    "    plt.plot(epochs, g_losses_per_epoch, label='G Loss', color='red')\n",
    "    plt.title('Generator and Discriminator Losses per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Third subplot for JSD with fixed y-axis\n",
    "    plt.subplot(3, 1, 3)\n",
    "    jsd_epochs = range(len(jsd_history))\n",
    "    plt.plot(jsd_epochs, jsd_history, 'g-', label='JSD Score')\n",
    "    plt.title('JSD Score Progress')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('JSD Score')\n",
    "    plt.ylim(0, 1)  # Set fixed y-axis range\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af8d16b3-ecf9-44a4-a8c2-539b4be2312d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [1/40]\n",
      "D_total_loss: 17.2631\n",
      "├─ Enc_loss: 2.0708\n",
      "├─ Dec_loss: 15.1923\n",
      "G_total_loss: 15.4225\n",
      "├─ G_global_loss: 1.0237\n",
      "└─ G_pixel_loss: 14.3988\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 13.7050\n",
      "├─ Enc_loss: 1.6322\n",
      "├─ Dec_loss: 12.0728\n",
      "G_total_loss: 2.6439\n",
      "├─ G_global_loss: 1.3341\n",
      "└─ G_pixel_loss: 1.3098\n",
      "\n",
      "Epoch [1/70] - Epoch Time: 33.97s - Total Time: 00:00:33 \n",
      "D_total_loss: 16.2801\n",
      "├─ Enc_loss: 1.6471\n",
      "├─ Dec_loss: 14.6330\n",
      "G_total_loss: 4.9420\n",
      "├─ G_global_loss: 1.3165\n",
      "└─ G_pixel_loss: 3.6255\n",
      "\n",
      "Latest JSD Score: 0.5493\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 10.9765\n",
      "├─ Enc_loss: 1.1809\n",
      "├─ Dec_loss: 9.7956\n",
      "G_total_loss: 4.3452\n",
      "├─ G_global_loss: 1.6581\n",
      "└─ G_pixel_loss: 2.6870\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 10.9960\n",
      "├─ Enc_loss: 0.5635\n",
      "├─ Dec_loss: 10.4325\n",
      "G_total_loss: 6.2885\n",
      "├─ G_global_loss: 2.6083\n",
      "└─ G_pixel_loss: 3.6802\n",
      "\n",
      "Epoch [2/70] - Epoch Time: 33.81s - Total Time: 00:01:07 \n",
      "D_total_loss: 10.5193\n",
      "├─ Enc_loss: 0.6766\n",
      "├─ Dec_loss: 9.8427\n",
      "G_total_loss: 6.4284\n",
      "├─ G_global_loss: 2.5941\n",
      "└─ G_pixel_loss: 3.8342\n",
      "\n",
      "Latest JSD Score: 0.5415\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.9348\n",
      "├─ Enc_loss: 0.2416\n",
      "├─ Dec_loss: 9.6932\n",
      "G_total_loss: 10.0225\n",
      "├─ G_global_loss: 3.6955\n",
      "└─ G_pixel_loss: 6.3270\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 12.3163\n",
      "├─ Enc_loss: 1.9596\n",
      "├─ Dec_loss: 10.3566\n",
      "G_total_loss: 6.3995\n",
      "├─ G_global_loss: 1.5250\n",
      "└─ G_pixel_loss: 4.8745\n",
      "\n",
      "Epoch [3/70] - Epoch Time: 33.73s - Total Time: 00:01:41 \n",
      "D_total_loss: 13.2189\n",
      "├─ Enc_loss: 1.5058\n",
      "├─ Dec_loss: 11.7131\n",
      "G_total_loss: 6.7800\n",
      "├─ G_global_loss: 1.7929\n",
      "└─ G_pixel_loss: 4.9871\n",
      "\n",
      "Latest JSD Score: nan\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 18.7605\n",
      "├─ Enc_loss: 1.1272\n",
      "├─ Dec_loss: 17.6332\n",
      "G_total_loss: 3.9674\n",
      "├─ G_global_loss: 3.0766\n",
      "└─ G_pixel_loss: 0.8908\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 10.6130\n",
      "├─ Enc_loss: 0.1758\n",
      "├─ Dec_loss: 10.4372\n",
      "G_total_loss: 6.6578\n",
      "├─ G_global_loss: 3.9876\n",
      "└─ G_pixel_loss: 2.6702\n",
      "\n",
      "Epoch [4/70] - Epoch Time: 33.79s - Total Time: 00:02:15 \n",
      "D_total_loss: 11.2749\n",
      "├─ Enc_loss: 0.3118\n",
      "├─ Dec_loss: 10.9632\n",
      "G_total_loss: 7.1576\n",
      "├─ G_global_loss: 3.8559\n",
      "└─ G_pixel_loss: 3.3017\n",
      "\n",
      "Latest JSD Score: 0.5074\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.0814\n",
      "├─ Enc_loss: 0.1010\n",
      "├─ Dec_loss: 8.9804\n",
      "G_total_loss: 7.6663\n",
      "├─ G_global_loss: 4.4712\n",
      "└─ G_pixel_loss: 3.1950\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 10.6718\n",
      "├─ Enc_loss: 0.7935\n",
      "├─ Dec_loss: 9.8783\n",
      "G_total_loss: 5.0244\n",
      "├─ G_global_loss: 2.9138\n",
      "└─ G_pixel_loss: 2.1105\n",
      "\n",
      "Epoch [5/70] - Epoch Time: 33.74s - Total Time: 00:02:49 \n",
      "D_total_loss: 11.5387\n",
      "├─ Enc_loss: 1.0513\n",
      "├─ Dec_loss: 10.4875\n",
      "G_total_loss: 7.1158\n",
      "├─ G_global_loss: 3.2799\n",
      "└─ G_pixel_loss: 3.8359\n",
      "\n",
      "Latest JSD Score: 0.6292\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.3396\n",
      "├─ Enc_loss: 0.7545\n",
      "├─ Dec_loss: 8.5852\n",
      "G_total_loss: 10.8867\n",
      "├─ G_global_loss: 5.4368\n",
      "└─ G_pixel_loss: 5.4499\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.2300\n",
      "├─ Enc_loss: 0.1654\n",
      "├─ Dec_loss: 9.0646\n",
      "G_total_loss: 9.4160\n",
      "├─ G_global_loss: 4.1090\n",
      "└─ G_pixel_loss: 5.3070\n",
      "\n",
      "Epoch [6/70] - Epoch Time: 35.21s - Total Time: 00:03:24 \n",
      "D_total_loss: 9.9839\n",
      "├─ Enc_loss: 0.3851\n",
      "├─ Dec_loss: 9.5988\n",
      "G_total_loss: 9.5666\n",
      "├─ G_global_loss: 4.4028\n",
      "└─ G_pixel_loss: 5.1637\n",
      "\n",
      "Latest JSD Score: 0.5182\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 10.2751\n",
      "├─ Enc_loss: 0.3400\n",
      "├─ Dec_loss: 9.9351\n",
      "G_total_loss: 10.8898\n",
      "├─ G_global_loss: 4.9922\n",
      "└─ G_pixel_loss: 5.8976\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 10.6161\n",
      "├─ Enc_loss: 0.0807\n",
      "├─ Dec_loss: 10.5354\n",
      "G_total_loss: 10.2752\n",
      "├─ G_global_loss: 5.9173\n",
      "└─ G_pixel_loss: 4.3579\n",
      "\n",
      "Epoch [7/70] - Epoch Time: 35.25s - Total Time: 00:03:59 \n",
      "D_total_loss: 10.4173\n",
      "├─ Enc_loss: 0.6133\n",
      "├─ Dec_loss: 9.8040\n",
      "G_total_loss: 10.1331\n",
      "├─ G_global_loss: 5.4575\n",
      "└─ G_pixel_loss: 4.6756\n",
      "\n",
      "Latest JSD Score: 0.6720\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.6539\n",
      "├─ Enc_loss: 0.1383\n",
      "├─ Dec_loss: 9.5156\n",
      "G_total_loss: 10.6138\n",
      "├─ G_global_loss: 5.9577\n",
      "└─ G_pixel_loss: 4.6561\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 8.8715\n",
      "├─ Enc_loss: 0.0560\n",
      "├─ Dec_loss: 8.8155\n",
      "G_total_loss: 10.3212\n",
      "├─ G_global_loss: 4.7676\n",
      "└─ G_pixel_loss: 5.5536\n",
      "\n",
      "Epoch [8/70] - Epoch Time: 35.34s - Total Time: 00:04:34 \n",
      "D_total_loss: 9.8237\n",
      "├─ Enc_loss: 0.2954\n",
      "├─ Dec_loss: 9.5283\n",
      "G_total_loss: 10.8810\n",
      "├─ G_global_loss: 5.2518\n",
      "└─ G_pixel_loss: 5.6292\n",
      "\n",
      "Latest JSD Score: 0.5301\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.5689\n",
      "├─ Enc_loss: 0.0268\n",
      "├─ Dec_loss: 9.5422\n",
      "G_total_loss: 9.8857\n",
      "├─ G_global_loss: 4.7515\n",
      "└─ G_pixel_loss: 5.1342\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 10.6130\n",
      "├─ Enc_loss: 0.0349\n",
      "├─ Dec_loss: 10.5781\n",
      "G_total_loss: 9.2421\n",
      "├─ G_global_loss: 5.6755\n",
      "└─ G_pixel_loss: 3.5666\n",
      "\n",
      "Epoch [9/70] - Epoch Time: 34.40s - Total Time: 00:05:09 \n",
      "D_total_loss: 10.0141\n",
      "├─ Enc_loss: 0.2512\n",
      "├─ Dec_loss: 9.7629\n",
      "G_total_loss: 12.6564\n",
      "├─ G_global_loss: 6.2035\n",
      "└─ G_pixel_loss: 6.4529\n",
      "\n",
      "Latest JSD Score: 0.6809\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.3008\n",
      "├─ Enc_loss: 0.0535\n",
      "├─ Dec_loss: 9.2473\n",
      "G_total_loss: 9.9386\n",
      "├─ G_global_loss: 4.5872\n",
      "└─ G_pixel_loss: 5.3514\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.6567\n",
      "├─ Enc_loss: 0.0188\n",
      "├─ Dec_loss: 9.6378\n",
      "G_total_loss: 13.3505\n",
      "├─ G_global_loss: 6.0672\n",
      "└─ G_pixel_loss: 7.2833\n",
      "\n",
      "Epoch [10/70] - Epoch Time: 33.89s - Total Time: 00:05:43 \n",
      "D_total_loss: 9.6390\n",
      "├─ Enc_loss: 0.0372\n",
      "├─ Dec_loss: 9.6018\n",
      "G_total_loss: 12.5429\n",
      "├─ G_global_loss: 5.8507\n",
      "└─ G_pixel_loss: 6.6922\n",
      "\n",
      "Latest JSD Score: 0.5273\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 8.9338\n",
      "├─ Enc_loss: 0.0102\n",
      "├─ Dec_loss: 8.9237\n",
      "G_total_loss: 14.3088\n",
      "├─ G_global_loss: 5.9407\n",
      "└─ G_pixel_loss: 8.3680\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 8.6155\n",
      "├─ Enc_loss: 0.0532\n",
      "├─ Dec_loss: 8.5623\n",
      "G_total_loss: 11.2631\n",
      "├─ G_global_loss: 6.1701\n",
      "└─ G_pixel_loss: 5.0931\n",
      "\n",
      "Epoch [11/70] - Epoch Time: 33.81s - Total Time: 00:06:16 \n",
      "D_total_loss: 9.5347\n",
      "├─ Enc_loss: 0.0523\n",
      "├─ Dec_loss: 9.4823\n",
      "G_total_loss: 13.6103\n",
      "├─ G_global_loss: 6.9860\n",
      "└─ G_pixel_loss: 6.6243\n",
      "\n",
      "Latest JSD Score: 0.7247\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.7921\n",
      "├─ Enc_loss: 1.2731\n",
      "├─ Dec_loss: 8.5190\n",
      "G_total_loss: 13.6684\n",
      "├─ G_global_loss: 5.0351\n",
      "└─ G_pixel_loss: 8.6333\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.6787\n",
      "├─ Enc_loss: 0.0052\n",
      "├─ Dec_loss: 9.6735\n",
      "G_total_loss: 13.9391\n",
      "├─ G_global_loss: 6.9722\n",
      "└─ G_pixel_loss: 6.9668\n",
      "\n",
      "Epoch [12/70] - Epoch Time: 34.25s - Total Time: 00:06:51 \n",
      "D_total_loss: 9.4155\n",
      "├─ Enc_loss: 0.0757\n",
      "├─ Dec_loss: 9.3398\n",
      "G_total_loss: 14.1086\n",
      "├─ G_global_loss: 7.1748\n",
      "└─ G_pixel_loss: 6.9338\n",
      "\n",
      "Latest JSD Score: 0.5680\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.5532\n",
      "├─ Enc_loss: 0.0027\n",
      "├─ Dec_loss: 9.5505\n",
      "G_total_loss: 15.2571\n",
      "├─ G_global_loss: 7.1007\n",
      "└─ G_pixel_loss: 8.1564\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.4848\n",
      "├─ Enc_loss: 0.0065\n",
      "├─ Dec_loss: 9.4783\n",
      "G_total_loss: 13.8640\n",
      "├─ G_global_loss: 7.4408\n",
      "└─ G_pixel_loss: 6.4232\n",
      "\n",
      "Epoch [13/70] - Epoch Time: 33.99s - Total Time: 00:07:25 \n",
      "D_total_loss: 9.4907\n",
      "├─ Enc_loss: 0.0161\n",
      "├─ Dec_loss: 9.4746\n",
      "G_total_loss: 15.2326\n",
      "├─ G_global_loss: 7.7299\n",
      "└─ G_pixel_loss: 7.5027\n",
      "\n",
      "Latest JSD Score: 0.6859\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.3138\n",
      "├─ Enc_loss: 0.0047\n",
      "├─ Dec_loss: 9.3090\n",
      "G_total_loss: 15.5473\n",
      "├─ G_global_loss: 8.5363\n",
      "└─ G_pixel_loss: 7.0109\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.4101\n",
      "├─ Enc_loss: 0.0012\n",
      "├─ Dec_loss: 9.4089\n",
      "G_total_loss: 15.2561\n",
      "├─ G_global_loss: 8.4484\n",
      "└─ G_pixel_loss: 6.8077\n",
      "\n",
      "Epoch [14/70] - Epoch Time: 33.86s - Total Time: 00:07:59 \n",
      "D_total_loss: 9.4020\n",
      "├─ Enc_loss: 0.0040\n",
      "├─ Dec_loss: 9.3981\n",
      "G_total_loss: 16.1047\n",
      "├─ G_global_loss: 8.1512\n",
      "└─ G_pixel_loss: 7.9535\n",
      "\n",
      "Latest JSD Score: 0.5935\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.1918\n",
      "├─ Enc_loss: 0.0025\n",
      "├─ Dec_loss: 9.1893\n",
      "G_total_loss: 17.9394\n",
      "├─ G_global_loss: 8.6739\n",
      "└─ G_pixel_loss: 9.2656\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.7187\n",
      "├─ Enc_loss: 0.0027\n",
      "├─ Dec_loss: 9.7159\n",
      "G_total_loss: 14.7767\n",
      "├─ G_global_loss: 6.7227\n",
      "└─ G_pixel_loss: 8.0540\n",
      "\n",
      "Epoch [15/70] - Epoch Time: 34.00s - Total Time: 00:08:33 \n",
      "D_total_loss: 9.2948\n",
      "├─ Enc_loss: 0.0328\n",
      "├─ Dec_loss: 9.2620\n",
      "G_total_loss: 12.8870\n",
      "├─ G_global_loss: 5.9415\n",
      "└─ G_pixel_loss: 6.9455\n",
      "\n",
      "Latest JSD Score: 0.8297\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 12.8690\n",
      "├─ Enc_loss: 0.0256\n",
      "├─ Dec_loss: 12.8434\n",
      "G_total_loss: 14.7817\n",
      "├─ G_global_loss: 9.4670\n",
      "└─ G_pixel_loss: 5.3146\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.6529\n",
      "├─ Enc_loss: 0.0012\n",
      "├─ Dec_loss: 9.6517\n",
      "G_total_loss: 18.1219\n",
      "├─ G_global_loss: 9.6614\n",
      "└─ G_pixel_loss: 8.4605\n",
      "\n",
      "Epoch [16/70] - Epoch Time: 33.89s - Total Time: 00:09:07 \n",
      "D_total_loss: 9.5383\n",
      "├─ Enc_loss: 0.0069\n",
      "├─ Dec_loss: 9.5314\n",
      "G_total_loss: 19.3688\n",
      "├─ G_global_loss: 9.9761\n",
      "└─ G_pixel_loss: 9.3927\n",
      "\n",
      "Latest JSD Score: 0.6083\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.2781\n",
      "├─ Enc_loss: 0.0006\n",
      "├─ Dec_loss: 9.2775\n",
      "G_total_loss: 18.6627\n",
      "├─ G_global_loss: 9.4430\n",
      "└─ G_pixel_loss: 9.2197\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.1603\n",
      "├─ Enc_loss: 0.0003\n",
      "├─ Dec_loss: 9.1599\n",
      "G_total_loss: 16.5806\n",
      "├─ G_global_loss: 8.8606\n",
      "└─ G_pixel_loss: 7.7200\n",
      "\n",
      "Epoch [17/70] - Epoch Time: 33.82s - Total Time: 00:09:40 \n",
      "D_total_loss: 9.1321\n",
      "├─ Enc_loss: 0.0006\n",
      "├─ Dec_loss: 9.1315\n",
      "G_total_loss: 17.3209\n",
      "├─ G_global_loss: 8.8558\n",
      "└─ G_pixel_loss: 8.4651\n",
      "\n",
      "Latest JSD Score: 0.7881\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 8.8356\n",
      "├─ Enc_loss: 0.1074\n",
      "├─ Dec_loss: 8.7282\n",
      "G_total_loss: 16.9922\n",
      "├─ G_global_loss: 8.6985\n",
      "└─ G_pixel_loss: 8.2937\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.2741\n",
      "├─ Enc_loss: 0.0005\n",
      "├─ Dec_loss: 9.2736\n",
      "G_total_loss: 19.2619\n",
      "├─ G_global_loss: 9.3278\n",
      "└─ G_pixel_loss: 9.9341\n",
      "\n",
      "Epoch [18/70] - Epoch Time: 33.88s - Total Time: 00:10:14 \n",
      "D_total_loss: 9.3149\n",
      "├─ Enc_loss: 0.0046\n",
      "├─ Dec_loss: 9.3104\n",
      "G_total_loss: 18.6779\n",
      "├─ G_global_loss: 9.3736\n",
      "└─ G_pixel_loss: 9.3043\n",
      "\n",
      "Latest JSD Score: 0.6219\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.6134\n",
      "├─ Enc_loss: 0.0004\n",
      "├─ Dec_loss: 9.6130\n",
      "G_total_loss: 17.4308\n",
      "├─ G_global_loss: 9.1497\n",
      "└─ G_pixel_loss: 8.2810\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 8.7872\n",
      "├─ Enc_loss: 0.0003\n",
      "├─ Dec_loss: 8.7868\n",
      "G_total_loss: 19.2950\n",
      "├─ G_global_loss: 8.5099\n",
      "└─ G_pixel_loss: 10.7850\n",
      "\n",
      "Epoch [19/70] - Epoch Time: 33.83s - Total Time: 00:10:48 \n",
      "D_total_loss: 9.3833\n",
      "├─ Enc_loss: 0.0434\n",
      "├─ Dec_loss: 9.3399\n",
      "G_total_loss: 18.0395\n",
      "├─ G_global_loss: 9.0610\n",
      "└─ G_pixel_loss: 8.9785\n",
      "\n",
      "Latest JSD Score: 0.7625\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.2569\n",
      "├─ Enc_loss: 0.0003\n",
      "├─ Dec_loss: 9.2566\n",
      "G_total_loss: 19.1445\n",
      "├─ G_global_loss: 9.7604\n",
      "└─ G_pixel_loss: 9.3841\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.9685\n",
      "├─ Enc_loss: 0.0002\n",
      "├─ Dec_loss: 9.9682\n",
      "G_total_loss: 20.8545\n",
      "├─ G_global_loss: 10.5194\n",
      "└─ G_pixel_loss: 10.3351\n",
      "\n",
      "Epoch [20/70] - Epoch Time: 33.84s - Total Time: 00:11:22 \n",
      "D_total_loss: 9.3654\n",
      "├─ Enc_loss: 0.0003\n",
      "├─ Dec_loss: 9.3652\n",
      "G_total_loss: 19.7055\n",
      "├─ G_global_loss: 10.1514\n",
      "└─ G_pixel_loss: 9.5541\n",
      "\n",
      "Latest JSD Score: 0.6449\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 8.5553\n",
      "├─ Enc_loss: 0.0003\n",
      "├─ Dec_loss: 8.5550\n",
      "G_total_loss: 19.4075\n",
      "├─ G_global_loss: 9.9012\n",
      "└─ G_pixel_loss: 9.5063\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 8.7851\n",
      "├─ Enc_loss: 0.0004\n",
      "├─ Dec_loss: 8.7847\n",
      "G_total_loss: 18.3891\n",
      "├─ G_global_loss: 9.1566\n",
      "└─ G_pixel_loss: 9.2325\n",
      "\n",
      "Epoch [21/70] - Epoch Time: 33.84s - Total Time: 00:11:56 \n",
      "D_total_loss: 9.2356\n",
      "├─ Enc_loss: 0.0003\n",
      "├─ Dec_loss: 9.2353\n",
      "G_total_loss: 18.9305\n",
      "├─ G_global_loss: 9.4119\n",
      "└─ G_pixel_loss: 9.5186\n",
      "\n",
      "Latest JSD Score: 0.8024\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 8.9049\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 8.9047\n",
      "G_total_loss: 20.1808\n",
      "├─ G_global_loss: 10.4070\n",
      "└─ G_pixel_loss: 9.7738\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.3920\n",
      "├─ Enc_loss: 0.0002\n",
      "├─ Dec_loss: 9.3919\n",
      "G_total_loss: 20.0384\n",
      "├─ G_global_loss: 10.6380\n",
      "└─ G_pixel_loss: 9.4004\n",
      "\n",
      "Epoch [22/70] - Epoch Time: 34.02s - Total Time: 00:12:30 \n",
      "D_total_loss: 9.1835\n",
      "├─ Enc_loss: 0.0002\n",
      "├─ Dec_loss: 9.1833\n",
      "G_total_loss: 19.7821\n",
      "├─ G_global_loss: 10.5975\n",
      "└─ G_pixel_loss: 9.1845\n",
      "\n",
      "Latest JSD Score: 0.6699\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 8.6637\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 8.6636\n",
      "G_total_loss: 19.1969\n",
      "├─ G_global_loss: 10.1555\n",
      "└─ G_pixel_loss: 9.0414\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.1438\n",
      "├─ Enc_loss: 0.0003\n",
      "├─ Dec_loss: 9.1435\n",
      "G_total_loss: 20.9331\n",
      "├─ G_global_loss: 9.6134\n",
      "└─ G_pixel_loss: 11.3198\n",
      "\n",
      "Epoch [23/70] - Epoch Time: 33.91s - Total Time: 00:13:04 \n",
      "D_total_loss: 9.3220\n",
      "├─ Enc_loss: 0.0002\n",
      "├─ Dec_loss: 9.3218\n",
      "G_total_loss: 19.0874\n",
      "├─ G_global_loss: 9.6582\n",
      "└─ G_pixel_loss: 9.4292\n",
      "\n",
      "Latest JSD Score: 0.8082\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.3677\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 9.3676\n",
      "G_total_loss: 19.1512\n",
      "├─ G_global_loss: 10.5255\n",
      "└─ G_pixel_loss: 8.6258\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.0164\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 9.0162\n",
      "G_total_loss: 20.5018\n",
      "├─ G_global_loss: 10.4461\n",
      "└─ G_pixel_loss: 10.0557\n",
      "\n",
      "Epoch [24/70] - Epoch Time: 33.91s - Total Time: 00:13:38 \n",
      "D_total_loss: 9.3678\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 9.3677\n",
      "G_total_loss: 20.1699\n",
      "├─ G_global_loss: 10.5355\n",
      "└─ G_pixel_loss: 9.6344\n",
      "\n",
      "Latest JSD Score: 0.6767\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.3729\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 9.3728\n",
      "G_total_loss: 21.2769\n",
      "├─ G_global_loss: 10.7114\n",
      "└─ G_pixel_loss: 10.5655\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.3732\n",
      "├─ Enc_loss: 0.0003\n",
      "├─ Dec_loss: 9.3730\n",
      "G_total_loss: 19.9808\n",
      "├─ G_global_loss: 9.4290\n",
      "└─ G_pixel_loss: 10.5518\n",
      "\n",
      "Epoch [25/70] - Epoch Time: 33.79s - Total Time: 00:14:11 \n",
      "D_total_loss: 9.3669\n",
      "├─ Enc_loss: 0.0002\n",
      "├─ Dec_loss: 9.3667\n",
      "G_total_loss: 19.6887\n",
      "├─ G_global_loss: 9.9880\n",
      "└─ G_pixel_loss: 9.7007\n",
      "\n",
      "Latest JSD Score: 0.8207\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 8.6543\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 8.6542\n",
      "G_total_loss: 22.5314\n",
      "├─ G_global_loss: 10.5635\n",
      "└─ G_pixel_loss: 11.9679\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.9614\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 9.9613\n",
      "G_total_loss: 20.2136\n",
      "├─ G_global_loss: 10.7644\n",
      "└─ G_pixel_loss: 9.4492\n",
      "\n",
      "Epoch [26/70] - Epoch Time: 33.86s - Total Time: 00:14:45 \n",
      "D_total_loss: 9.1268\n",
      "├─ Enc_loss: 0.0002\n",
      "├─ Dec_loss: 9.1266\n",
      "G_total_loss: 20.5086\n",
      "├─ G_global_loss: 10.6583\n",
      "└─ G_pixel_loss: 9.8503\n",
      "\n",
      "Latest JSD Score: 0.6926\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.4874\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 9.4873\n",
      "G_total_loss: 19.7054\n",
      "├─ G_global_loss: 10.5379\n",
      "└─ G_pixel_loss: 9.1675\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.1362\n",
      "├─ Enc_loss: 0.0002\n",
      "├─ Dec_loss: 9.1359\n",
      "G_total_loss: 19.0022\n",
      "├─ G_global_loss: 8.3625\n",
      "└─ G_pixel_loss: 10.6397\n",
      "\n",
      "Epoch [27/70] - Epoch Time: 33.97s - Total Time: 00:15:19 \n",
      "D_total_loss: 9.3251\n",
      "├─ Enc_loss: 0.0002\n",
      "├─ Dec_loss: 9.3249\n",
      "G_total_loss: 19.8096\n",
      "├─ G_global_loss: 9.4888\n",
      "└─ G_pixel_loss: 10.3208\n",
      "\n",
      "Latest JSD Score: 0.8114\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.6008\n",
      "├─ Enc_loss: 0.0005\n",
      "├─ Dec_loss: 9.6003\n",
      "G_total_loss: 20.3774\n",
      "├─ G_global_loss: 11.2289\n",
      "└─ G_pixel_loss: 9.1486\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 8.5393\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 8.5392\n",
      "G_total_loss: 21.0010\n",
      "├─ G_global_loss: 10.6846\n",
      "└─ G_pixel_loss: 10.3164\n",
      "\n",
      "Epoch [28/70] - Epoch Time: 34.11s - Total Time: 00:15:53 \n",
      "D_total_loss: 9.3078\n",
      "├─ Enc_loss: 0.0002\n",
      "├─ Dec_loss: 9.3077\n",
      "G_total_loss: 20.6646\n",
      "├─ G_global_loss: 10.8357\n",
      "└─ G_pixel_loss: 9.8289\n",
      "\n",
      "Latest JSD Score: 0.7081\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 9.1268\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 9.1267\n",
      "G_total_loss: 20.2026\n",
      "├─ G_global_loss: 10.6543\n",
      "└─ G_pixel_loss: 9.5483\n",
      "\n",
      "Batch [21/40]\n",
      "D_total_loss: 9.1192\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 9.1191\n",
      "G_total_loss: 19.7713\n",
      "├─ G_global_loss: 10.5315\n",
      "└─ G_pixel_loss: 9.2399\n",
      "\n",
      "Epoch [29/70] - Epoch Time: 34.42s - Total Time: 00:16:28 \n",
      "D_total_loss: 9.3577\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 9.3575\n",
      "G_total_loss: 20.4107\n",
      "├─ G_global_loss: 10.5427\n",
      "└─ G_pixel_loss: 9.8680\n",
      "\n",
      "Latest JSD Score: 0.7901\n",
      "--------------------------------------------------\n",
      "Batch [1/40]\n",
      "D_total_loss: 8.6522\n",
      "├─ Enc_loss: 0.0001\n",
      "├─ Dec_loss: 8.6521\n",
      "G_total_loss: 21.1749\n",
      "├─ G_global_loss: 10.9914\n",
      "└─ G_pixel_loss: 10.1835\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m iteration_losses, total_iterations, jsd_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m70\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m plot_metrics(\n\u001b[0;32m      5\u001b[0m     iteration_losses\u001b[38;5;241m=\u001b[39miteration_losses,\n\u001b[0;32m      6\u001b[0m     jsd_history\u001b[38;5;241m=\u001b[39mjsd_history,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     dataloader_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[0;32m     10\u001b[0m )\n",
      "Cell \u001b[1;32mIn[16], line 168\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(generator, discriminator, dataloader, num_epochs, d_step, g_step)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Train discriminator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_step):\n\u001b[1;32m--> 168\u001b[0m     d_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreal_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreal_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mfake_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfake_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmixed_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmixed_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_optimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# Sum up d_losses\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m d_losses:\n",
      "Cell \u001b[1;32mIn[16], line 56\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(discriminator, real_sequences, fake_sequences, mixed_sequences, mask, optimizer)\u001b[0m\n\u001b[0;32m     51\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(discriminator\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     53\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menc_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43menc_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdec_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: dec_loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: total_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     59\u001b[0m }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iteration_losses, total_iterations, jsd_history = train(generator, discriminator, dataloader,\n",
    "                                                       num_epochs=70, d_step=1, g_step=1)\n",
    "\n",
    "plot_metrics(\n",
    "    iteration_losses=iteration_losses,\n",
    "    jsd_history=jsd_history,\n",
    "    total_iterations=total_iterations,\n",
    "    num_epochs=70,\n",
    "    dataloader_size=len(dataloader)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
